% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\author{}
\date{}

\begin{document}

\hypertarget{a-study-on-the-hierarchical-structure-of-knowledge-and-the-cognition-of-ignorance}{%
\section{A Study on the Hierarchical Structure of Knowledge and the
Cognition of
Ignorance}\label{a-study-on-the-hierarchical-structure-of-knowledge-and-the-cognition-of-ignorance}}

\emph{Kunihiro Sugiyama}\\
kunihiros@gmail.com

\hypertarget{introduction}{%
\subsection{Introduction}\label{introduction}}

Human knowledge possesses a multi-layered structure, and the ability to
recognize one's own ignorance (metacognition) is crucial for learning
and decision-making. This study proposes a recursive model of knowledge
and ignorance based on a single core function: \textbf{\(K\)}, which
represents epistemic recognition. By applying this function
recursively---\(K(x)\), \(K(K(x))\), \(K(K(K(x)))\), and so on---we
formalize the hierarchical structure of self-awareness that
distinguishes \textbf{Socratic wisdom} (``knowing that one does not
know'') from the \textbf{Dunning-Kruger effect} (``not knowing that one
does not know'').

Kant (1781), in his \emph{Critique of Pure Reason}, posed the
foundational question of epistemology: what are the limits of human
cognition, and can reason examine itself? His answer---that reason must
critique reason---established the recursive structure of self-reflection
as a philosophical problem. Yet Kant's contribution was
\textbf{descriptive}: he demonstrated that limits exist, but provided no
apparatus for locating their precise coordinates or guiding their
correction.

This study provides what classical epistemology could not: a
\textbf{mathematical framework} that transforms the Kantian question
from philosophical meditation into \textbf{operational methodology}. The
function \(K\) does not merely describe where cognition fails---it
provides the coordinates for \textbf{targeted intervention}. By
representing epistemic states on a continuous scale, we gain the
capacity to \textbf{observe the phenomenon of intelligence itself, and
to intervene in its structure}. This framework opens the possibility of
not only understanding cognition but \textbf{actively shaping its
trajectory toward new forms of knowing}.

This model integrates insights from \textbf{metacognition research},
\textbf{epistemology}, and \textbf{type theory} to address three aspects
that have not been sufficiently unified in existing research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{recursive nature of self-awareness}: The same epistemic
  question (``Do I know?'') can be applied at every level of reflection.
\item
  The \textbf{continuous gradation of knowledge}: Knowledge states exist
  on a continuum from complete misconception (\(-1\)) through ignorance
  (\(0\)) to accurate knowledge (\(1\)).
\item
  The \textbf{distinction between epistemic state and phenomenological
  confidence}: What one knows versus how certain one feels are
  orthogonal dimensions.
\end{enumerate}

By presenting a mathematically rigorous yet philosophically grounded
framework, this study seeks to deepen our understanding of the structure
of knowledge and the cognitive mechanisms of ignorance.

\hypertarget{contribution-a-conceptual-foundation}{%
\subsubsection{Contribution: A Conceptual
Foundation}\label{contribution-a-conceptual-foundation}}

This paper establishes the \textbf{conceptual foundation} for a unified
theory of recursive metacognition. The trichotomy of epistemic
states---\textbf{knowing}, \textbf{not knowing}, and
\textbf{misunderstanding}---is a universal human experience that
transcends cultures, domains, and disciplines. Before elaborating
measurement-theoretic models or conducting empirical validation, we must
first \textbf{settle the conceptual vocabulary}.

\textbf{What this paper provides:} 1. A \textbf{single, unified operator
\(K\)} that applies recursively at all levels of self-reflection 2. A
\textbf{purely observational framework} where \(K\) is not a mental
process but an \textbf{observation protocol} 3. A \textbf{complete
taxonomy} (27 patterns) that classifies all possible metacognitive
configurations 4. A \textbf{resolution of apparent contradictions}
(e.g., \(K(0) = 0\) vs \(K_1 = -1\)) through layer independence

\textbf{What this paper deliberately does not provide:} - Probabilistic
measurement models (future work) - Empirical validation (orthogonal
contribution) - Formal type-theoretic proofs (analogical treatment
suffices for conceptual clarity)

These omissions are not gaps but \textbf{scope boundaries}.
Measurement-theoretic elaboration and empirical validation require this
conceptual foundation to be settled first. We invite the research
community to build upon this foundation.

\hypertarget{executive-summary-framework-at-a-glance}{%
\subsection{Executive Summary: Framework at a
Glance}\label{executive-summary-framework-at-a-glance}}

This section provides a concise overview of the framework's core
components for readers seeking quick orientation.

\hypertarget{core-apparatus}{%
\subsubsection{Core Apparatus}\label{core-apparatus}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3438}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2812}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Component
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Purpose
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(K_n\)} & Observation function at layer \(n\) & Maps
State\(_n\) to \([-1, 1]\) \\
\textbf{State\(_n\)} & Epistemic state object at layer \(n\) & Target of
observation \\
\textbf{\(f_n\)} & State function & Computes State\(_n\) from inputs \\
\textbf{\(g_n\)} & Embedding map & Maps categorical State\(_n\) to
\(K_n \in [-1, 1]\) \\
\end{longtable}

\hypertarget{anchor-semantics-all-layers}{%
\subsubsection{Anchor Semantics (All
Layers)}\label{anchor-semantics-all-layers}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1400}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3400}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5200}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Layer 0 Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Layer \(n \geq 1\) Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_n = +1\) & Knowledge (correct response) & Alignment (accurate
self-assessment) \\
\(K_n = 0\) & Ignorance (no response / ``I don't know'') & Indeterminacy
(uncertain self-assessment) \\
\(K_n = -1\) & Misconception (incorrect response) & Misalignment
(inaccurate self-assessment) \\
\end{longtable}

\hypertarget{pattern-taxonomy-preview}{%
\subsubsection{27-Pattern Taxonomy
(Preview)}\label{pattern-taxonomy-preview}}

The framework generates \(3 \times 3 \times 3 = 27\) metacognitive
patterns from combinations of \((K_0, K_1, K_2) \in \{-1, 0, +1\}^3\).
The complete enumeration appears in Section ``Complete Taxonomy of 27
Metacognitive Patterns.'' Key patterns include:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1915}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4043}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1277}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2766}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\((K_0, K_1, K_2)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\#14 & \((0, +1, +1)\) & \textbf{Socratic Wisdom} & Knows that they
don't know \\
\#5 & \((-1, -1, -1)\) & \textbf{Dunning-Kruger (Deep)} & Wrong,
overconfident, unaware \\
\#22 & \((+1, -1, +1)\) & \textbf{Imposter Syndrome (Aware)} & Correct
but self-doubting, aware of this tendency \\
\end{longtable}

\hypertarget{correspondence-with-established-metrics}{%
\subsubsection{Correspondence with Established
Metrics}\label{correspondence-with-established-metrics}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2264}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3774}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2642}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Established Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relationship
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & First-order accuracy & IRT ability \(\theta\) &
\(K_0 \approx \tanh(\theta)\) \\
\(K_1\) & Metacognitive alignment & meta-d'/d' &
\(K_1 \approx \tanh(\text{meta-d}'/2)\) \\
\(K_1\) & Calibration & ECE & \(K_1 \approx 1 - 2 \cdot \text{ECE}\) \\
\(K_2\) & Meta-metacognitive stability & Test-retest \(K_1\) & Novel
contribution \\
\end{longtable}

\emph{See Related Work section for detailed correspondence analysis.}

\hypertarget{latent-variable-model-preview}{%
\subsubsection{Latent Variable Model
(Preview)}\label{latent-variable-model-preview}}

For continuous \(K_n\) estimation, we employ a latent variable model:

\[K_n^* \sim \mathcal{N}(\mu_n, \sigma_n^2)\]
\[K_n = \begin{cases} +1 & \text{if } K_n^* > \tau^+ \\ 0 & \text{if } \tau^- \leq K_n^* \leq \tau^+ \\ -1 & \text{if } K_n^* < \tau^- \end{cases}\]

\emph{See Measurement Theory section for full specification.}

\hypertarget{philosophical-foundation-and-interpretive-notes}{%
\subsection{Philosophical Foundation and Interpretive
Notes}\label{philosophical-foundation-and-interpretive-notes}}

This section clarifies the philosophical motivation behind this paper
and provides essential interpretive guidance to prevent misunderstanding
of the proposed model.

\hypertarget{theoretical-rationale}{%
\subsubsection{Theoretical Rationale}\label{theoretical-rationale}}

This study is grounded in the logical structure of recursive ignorance,
exemplified by the proposition \textbf{``I don't know what I don't
know.''} If ``knowing one's ignorance'' (Socratic wisdom) is a
recognized concept, then logically, ``not knowing one's ignorance'' must
also exist. And if that exists, then so must ``not knowing that one
doesn't know one's ignorance''---and so on, recursively.

The goal of this paper is to \textbf{mathematically formalize this
recursive structure of knowledge and ignorance}, not to judge or rank
cognitive states.

\hypertarget{descriptive-nature-of-the-scale}{%
\subsubsection{Descriptive Nature of the
Scale}\label{descriptive-nature-of-the-scale}}

The values \(-1\), \(0\), and \(1\) in this model function as
\textbf{epistemic state descriptors}. They serve as epistemic
coordinates rather than normative metrics (e.g., ``good'' or ``bad'').

\begin{longtable}[]{@{}cl@{}}
\toprule\noalign{}
Value & Meaning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) & The subject holds correct knowledge. \\
\(0\) & The subject lacks knowledge (ignorance). \\
\(-1\) & The subject holds incorrect knowledge (misconception). \\
\end{longtable}

A subject in state \(-1\) (misconception) is not normatively inferior to
a subject in state \(0\) (ignorance); they occupy \textbf{distinct
epistemic loci}. Whether one state is ``preferable'' to another depends
on context, goals, and values---domains outside the scope of this model.

\hypertarget{separation-of-knowledge-and-confidence}{%
\subsubsection{Separation of Knowledge and
Confidence}\label{separation-of-knowledge-and-confidence}}

A fundamental distinction in this framework is that \textbf{the function
\(K\) measures epistemic state, not phenomenological confidence}.
Confidence is a separate dimension that will be introduced later in the
measurement section.

\begin{itemize}
\tightlist
\item
  \(K(x)\): How accurately the subject recognizes object \(x\)
  (epistemic state).
\item
  \(C\) (Confidence): How certain the subject feels about their
  recognition (phenomenological experience).
\end{itemize}

This separation is essential for capturing phenomena like the
Dunning-Kruger effect, where \(K(x) = 0\) (the subject does not know)
but \(K(K(x)) = -1\) (the subject misrecognizes their ignorance), often
accompanied by high subjective confidence.

\hypertarget{operational-boundary-k_1-vs-confidence}{%
\paragraph{\texorpdfstring{Operational Boundary: \(K_1\) vs
Confidence}{Operational Boundary: K\_1 vs Confidence}}\label{operational-boundary-k_1-vs-confidence}}

A natural question arises: if both \(K_1\) and confidence \(C\) assess
subjective epistemic states, how do they differ operationally? The
distinction lies in \textbf{timing}, \textbf{information basis}, and
\textbf{what is measured}.

\textbf{Conceptual Distinction:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1944}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Dimension
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Confidence (\(C\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_1\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Timing} & Before or during response & After feedback \\
\textbf{Information Basis} & No external reference & Correctness
revealed \\
\textbf{What is Measured} & Subjective certainty & Meta-cognitive
accuracy \\
\textbf{Definition} & ``How sure am I?'' & ``Given my answer was
{[}correct/incorrect{]}, was my claim appropriate?'' \\
\end{longtable}

\textbf{Operational Distinction:}

\begin{itemize}
\item
  \textbf{Confidence \(C\)}: Collected at \(t_1\) (during response),
  before feedback. The subject reports certainty without knowing
  correctness. This is a phenomenological measure.
\item
  \textbf{\(K_1\) (Meta-Accuracy)}: Assessed at \(t_2\) (after
  feedback), comparing the claim (``I know''/``I don't know'') to
  revealed correctness. This is a behavioral measure of calibration.
\end{itemize}

\textbf{Why Both Are Needed---Divergence Scenarios:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1786}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1607}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Claim
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Confidence \(C\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Calibrated Expert & \(+1\) & ``I know'' & \(+1\) & High & All aligned \\
Overconfident Novice & \(-1\) & ``I know'' & \(-1\) & High &
\(K_1 \ne C\) direction \\
Underconfident Expert & \(+1\) & ``I don't know'' & \(-1\) & Low &
\(K_1 \ne C\) direction \\
Appropriate Uncertainty & \(0\) & ``I don't know'' & \(+1\) & Low &
Calibrated ignorance \\
\end{longtable}

The overconfident novice and underconfident expert both show
\(K_1 = -1\) (meta-miscalibration) but with opposite confidence
levels---demonstrating that \(K_1\) and \(C\) carry independent
information.

\textbf{Joint Model \((K_0, K_1, C)\):}

When the full triple is observed, richer patterns emerge:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2143}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4048}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3810}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\((K_0, K_1, C)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Dunning-Kruger & \((-1, -1, \text{High})\) & Wrong, claims knowing,
confident \\
Impostor Syndrome & \((+1, -1, \text{Low})\) & Right, claims not
knowing, unconfident \\
Calibrated Competence & \((+1, +1, \text{High})\) & Right, claims
knowing, appropriately confident \\
Calibrated Uncertainty & \((0, +1, \text{Low})\) & Uncertain, claims not
knowing, appropriately unconfident \\
Anxious Accuracy & \((+1, +1, \text{Low})\) & Right, claims knowing, but
feels unconfident \\
\end{longtable}

This joint model reveals that \textbf{Dunning-Kruger and Impostor
Syndrome are symmetric patterns} in the \((K_1, C)\) space, while
\(K_0\) distinguishes their actual competence. The independence of
\(K_1\) and \(C\) is thus not merely conceptual but empirically testable
through dissociation patterns.

\hypertarget{scope-clarification-methodological-relativism}{%
\subsection{Scope Clarification: Methodological
Relativism}\label{scope-clarification-methodological-relativism}}

This section clarifies the scope and philosophical stance of this
framework.

\hypertarget{what-this-model-does}{%
\subsubsection{What This Model Does}\label{what-this-model-does}}

This model provides a \textbf{mathematical apparatus} for representing
and manipulating the \textbf{structure of epistemic states} relative to
a proposition. The function \(K(x)\) measures the subject's epistemic
state regarding proposition \(x\):

\begin{longtable}[]{@{}cll@{}}
\toprule\noalign{}
\(K(x)\) & State & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) & Aligned & Subject's belief is consistent with \(x\) \\
\(0\) & Indeterminate & Subject has no determinate stance on \(x\) \\
\(-1\) & Opposed & Subject's belief is contrary to \(x\) \\
\end{longtable}

\hypertarget{what-this-model-does-not-do}{%
\subsubsection{What This Model Does Not
Do}\label{what-this-model-does-not-do}}

This model \textbf{does not adjudicate} what is ``correct'' or ``true.''
The designation of a proposition as the ``target'' (such that
\(K(x) = 1\) represents success) is a \textbf{methodological choice}
made by the experimenter, not a claim of this framework.

The proposition \(x\) itself serves as the \textbf{implicit reference
point}. What counts as ``aligned'' (\(K(x) = 1\)) versus ``opposed''
(\(K(x) = -1\)) is determined by the \textbf{experimental context}
(e.g., expert consensus, empirical measurement, community agreement),
not by this model.

\hypertarget{methodological-relativism}{%
\subsubsection{Methodological
Relativism}\label{methodological-relativism}}

This framework adopts a position of \textbf{methodological relativism}:
the reference point is necessarily context-dependent, and the model
operates on the structure of epistemic states relative to that chosen
reference. This design allows researchers with different philosophical
commitments (realism, relativism, pragmatism) to use the same
mathematical framework while maintaining their preferred interpretation
of what constitutes ``correct'' knowledge.

\hypertarget{reconciling-objectivity-and-methodological-relativism}{%
\subsubsection{Reconciling Objectivity and Methodological
Relativism}\label{reconciling-objectivity-and-methodological-relativism}}

\textbf{The Apparent Tension:}

This framework claims both: 1. \textbf{Objective evaluation}: \(K_n\)
values are determined by observable behavior, not subjective judgment 2.
\textbf{Methodological relativism}: The reference standard (``correct''
answer) is context-dependent

\textbf{Resolution: Objectivity as Operational Repeatability}

``Objective'' in this framework means \textbf{operationally repeatable
given a reference}:

\[\text{Objectivity} := \text{Repeatability} \mid \text{Reference}\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4706}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5294}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Reference} & The designated ground truth (e.g., expert
consensus, textbook answer) \\
\textbf{Procedure} & The MAT protocol (response -\textgreater{} claim
-\textgreater{} comparison) \\
\textbf{Repeatability} & Different observers applying the same procedure
to the same data obtain the same \(K_n\) \\
\end{longtable}

\textbf{The Relativism:}

The \textbf{choice of reference} is not determined by the framework.
Different communities may designate different references: - Scientific
community: Peer-reviewed consensus - Educational setting: Curriculum
standards - Clinical context: Diagnostic criteria

\textbf{What the Framework Provides:}

Once a reference is designated, the framework provides: 1.
\textbf{Deterministic scoring}: Response + Claim + Reference
-\textgreater{} \(K_n\) (via \(f_n\) and \(g_n\)) 2.
\textbf{Cross-context comparability}: Same \(K_n\) semantics across
different domains 3. \textbf{Transparency}: All assumptions (reference,
\(f_n\), thresholds) are explicit

\textbf{Handling Reference Uncertainty:}

When the reference itself is uncertain or contested:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3846}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6154}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Implementation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Probabilistic reference} & Model reference as a distribution;
report \(E[K_n]\) and uncertainty bounds \\
\textbf{Sensitivity analysis} & Report \(K_n\) under multiple plausible
references \\
\textbf{Higher-layer encoding} & Treat reference uncertainty as a
property of State\(_{n+1}\) \\
\end{longtable}

\textbf{Scope Boundary:} Full treatment of contested references (e.g.,
scientific controversies) is beyond the current framework's scope and
marked for future development.

\hypertarget{the-recursive-structure-kkkx}{%
\subsection{\texorpdfstring{The Recursive Structure:
\(K(K(K(x)))\)}{The Recursive Structure: K(K(K(x)))}}\label{the-recursive-structure-kkkx}}

The cognitive structure of knowledge is modeled using a
\textbf{recursive epistemic function} \(K\). This model formalizes the
intuition that the same question---``Do I know?''---can be applied at
every level of self-reflection.

\hypertarget{formal-definition-of-k}{%
\subsubsection{\texorpdfstring{Formal Definition of
\(K\)}{Formal Definition of K}}\label{formal-definition-of-k}}

We adopt an \textbf{observational family} interpretation of \(K\), which
provides a clear and consistent framework for understanding recursive
metacognition.

\hypertarget{formal-framework-layered-observation-model}{%
\paragraph{Formal Framework: Layered Observation
Model}\label{formal-framework-layered-observation-model}}

\textbf{Definition (Observation Family):}

Let \(\{K^{(n)}\}_{n=0}^{\infty}\) be a family of observation functions,
where each \(K^{(n)}\) maps from a layer-specific state space to the
epistemic scale \([-1, 1]\):

\[K^{(n)}: \mathcal{S}_n \to [-1, 1]\]

\textbf{Definition (State Hierarchy):}

\begin{itemize}
\tightlist
\item
  \(\mathcal{S}_0\): First-order epistemic states (correctness of
  responses)
\item
  \(\mathcal{S}_1\): Metacognitive states (alignment between claims and
  \(\mathcal{S}_0\))
\item
  \(\mathcal{S}_n\): n-th order states (alignment between claims and
  \(\mathcal{S}_{n-1}\))
\end{itemize}

\textbf{Notational Convention:}

The notation \(K(K(x))\) is a \textbf{shorthand} for
\(K^{(1)}(\text{State}_1(x))\), not numerical composition.

More precisely: - \(K_0(x) := K^{(0)}(\text{State}_0(x))\) -
\(K_1(x) := K^{(1)}(\text{State}_1(x))\) -
\(K_n(x) := K^{(n)}(\text{State}_n(x))\)

\textbf{Shared Anchor Semantics:}

All \(K^{(n)}\) share the same anchor constraints: -
\(K^{(n)}(\text{"knowledge"}) = 1\) -
\(K^{(n)}(\text{"ignorance"}) = 0\) -
\(K^{(n)}(\text{"misconception"}) = -1\)

This ensures cross-layer comparability while allowing distinct
measurement procedures per layer.

\hypertarget{dual-notation-system-symbolic-vs-formal}{%
\paragraph{Dual Notation System: Symbolic vs
Formal}\label{dual-notation-system-symbolic-vs-formal}}

This paper employs two complementary notational systems that serve
distinct purposes:

\textbf{1. Symbolic Notation: \(K(K(K(x)))\)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Philosophical intuition and rhetorical clarity \\
\textbf{Meaning} & The recursive structure of ``not knowing that one
does not know'' \\
\textbf{Origin} & The paper's foundational insight: ignorance of
ignorance of ignorance \\
\textbf{Usage} & Introduction, motivation, conceptual discussion,
examples \\
\textbf{Status} & Evocative shorthand, NOT operational definition \\
\end{longtable}

\textbf{2. Formal Notation: \(K_n(x)\)}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Purpose} & Mathematical rigor and operational precision \\
\textbf{Definition} &
\(K_n(x) := K^{(n)}(\text{State}_n(x)) = \hat{K}(g_n(\text{State}_n(x)))\) \\
\textbf{Usage} & Definitions, axioms, measurement protocols, empirical
analysis \\
\textbf{Status} & Operational definition for all formal purposes \\
\end{longtable}

\textbf{Critical Distinction:}

The symbolic notation \(K(K(x))\) is \textbf{NOT} numerical composition
(i.e., \(K\) applied to its own output value). It is a
\textbf{rhetorical device} representing the philosophical insight that
the same epistemic question (``Do I know?'') applies recursively at
every level of reflection.

\[K(K(x)) \not\equiv K(K_0(x)) \quad \text{(NOT numerical composition)}\]
\[K(K(x)) \equiv K_1(x) \quad \text{(notational equivalence only)}\]

\textbf{For all formal purposes---definitions, proofs, measurement,
analysis---use \(K_n(x)\) exclusively.}

\textbf{Why Both?}

\begin{itemize}
\tightlist
\item
  \textbf{\(K(K(K(x)))\)} captures the \emph{philosophical essence}: the
  infinite regress of self-reflection
\item
  \textbf{\(K_n(x)\)} enables \emph{mathematical precision}:
  layer-specific observation with distinct objects
\end{itemize}

This dual system parallels established practice: - Physics: \(E = mc^2\)
(iconic) vs tensor formulation (operational) - Calculus:
\(\frac{dy}{dx}\) (Leibniz, intuitive) vs \(\lim_{h \to 0}\) (rigorous)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{DEFINITION (Core Formal Apparatus):}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{State objects}: \(\text{State}_n \in \{+1, 0, -1\}\) for each
  layer \(n\)
\item
  \textbf{State functions}:
  \(f_n: (\text{State}_{n-1}, \text{Claim}_n) \to \text{State}_n\)
\item
  \textbf{Embedding maps}: \(g_n: \text{State}_n \to [-1, 1]\)
\item
  \textbf{Observation}: \(K_n(x) = \hat{K}(g_n(\text{State}_n(x)))\)
\item
  \textbf{Anchor constraints}: \(\hat{K}(-1) = -1\), \(\hat{K}(0) = 0\),
  \(\hat{K}(+1) = +1\)
\end{enumerate}

\textbf{All formal work in this paper operates within this apparatus.}
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{unified-formalization-via-embedding-maps}{%
\paragraph{Unified Formalization via Embedding
Maps}\label{unified-formalization-via-embedding-maps}}

To resolve the apparent type ambiguity between
\(K^{(n)}: \mathcal{S}_n \to [-1, 1]\) and the recursive
\(K: [-1, 1] \to [-1, 1]\), we introduce \textbf{embedding maps} that
convert categorical states to the continuous scale.

\textbf{Definition (Embedding Maps):}

Each layer has an embedding map \(g_n\) that converts categorical states
to the continuous scale:

\[g_n: \mathcal{S}_n \to [-1, 1]\]

Where: -
\(g_0: \{\text{correct}, \text{incorrect}, \text{absent}\} \to \{1, -1, 0\}\)
-
\(g_1: \{\text{aligned}, \text{misaligned}, \text{uncertain}\} \to \{1, -1, 0\}\)
-
\(g_n: \{\text{aligned}, \text{misaligned}, \text{uncertain}\} \to \{1, -1, 0\}\)
for \(n \geq 1\)

\textbf{State\(_0\) Canonical Mapping:}

\begin{longtable}[]{@{}lcl@{}}
\toprule\noalign{}
\(f_0\) Output & \(g_0\) Value & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
correct & 1 & Response matches reference \\
incorrect & -1 & Response contradicts reference \\
absent & 0 & No response / ``I don't know'' \\
\end{longtable}

\textbf{Clarification:} ``Ignorance'' in the sense of \(K_0 = 0\) means
\textbf{absence of determinate stance}, not ``being wrong.'' A wrong
answer is a \textbf{misconception} (\(K_0 = -1\)), not ignorance.

\textbf{Definition (Unified Observation Scorer):}

After embedding, a single scorer \(\hat{K}\) operates on the embedded
space:

\[\hat{K}: [-1, 1] \to [-1, 1]\]

With the identity mapping for prototypical anchors: \(\hat{K}(1) = 1\),
\(\hat{K}(0) = 0\), \(\hat{K}(-1) = -1\).

\textbf{Composition:}

\[K_n(x) = \hat{K}(g_n(\text{State}_n(x)))\]

\hypertarget{role-of-the-unified-scorer-hatk}{%
\paragraph{\texorpdfstring{Role of the Unified Scorer
\(\hat{K}\)}{Role of the Unified Scorer \textbackslash hat\{K\}}}\label{role-of-the-unified-scorer-hatk}}

\textbf{Question: Is \(\hat{K}\) Necessary?}

Given that: - \(g_n: \mathcal{S}_n \to [-1, 1]\) embeds state spaces
into the common scale - Anchors are mapped to \(\{-1, 0, +1\}\) by
\(g_n\)

Is an additional \(\hat{K}: [-1, 1] \to [-1, 1]\) needed?

\textbf{Answer: \(\hat{K}\) Serves Three Functions}

\textbf{Function 1: Anchor Preservation Guarantee}

\[\hat{K}(g_n(\text{anchor})) = g_n(\text{anchor}) \quad \text{for anchors } \in \{-1, 0, +1\}\]

This ensures that regardless of how \(g_n\) handles intermediate values,
the anchor semantics are preserved.

\textbf{Function 2: Cross-Layer Normalization}

If different layers use different \(g_n\) with varying intermediate
behaviors:

\[\hat{K} \circ g_n \text{ ensures comparability across layers}\]

\textbf{Function 3: Monotonicity Enforcement}

\(\hat{K}\) can enforce global monotonicity even if \(g_n\) has local
non-monotonicities:

\[x < y \Rightarrow \hat{K}(x) \leq \hat{K}(y)\]

\hypertarget{specification-of-hatk}{%
\paragraph{\texorpdfstring{Specification of
\(\hat{K}\)}{Specification of \textbackslash hat\{K\}}}\label{specification-of-hatk}}

\textbf{Default Choice: Identity}

For most applications:

\[\hat{K}(x) = x\]

Under this choice, \(\hat{K}\) is formally present but operationally
redundant.

\textbf{Non-Trivial Choice: Anchor-Preserving Power Function}

For applications requiring stronger intermediate compression:

\[\hat{K}(x) = \text{sign}(x) \cdot |x|^\gamma, \quad \gamma \in (0, 1]\]

This preserves anchors (\(\hat{K}(\pm 1) = \pm 1\), \(\hat{K}(0) = 0\))
while compressing intermediate values.

\textbf{Selection Criterion}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3611}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6389}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Application
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Recommended \(\hat{K}\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Categorical \(K_n\) only & Identity \\
Continuous \(K_n\) with calibration metrics & Identity \\
Continuous \(K_n\) with arbitrary \(g_n\) & Anchor-preserving power
function \\
\end{longtable}

\textbf{Formal Definition (Updated)}:

\[K_n = \hat{K}(g_n(f_n(\text{Response}_n, \text{Claim}_n, \text{State}_{n-1})))\]

With default \(\hat{K} = \text{identity}\):

\[K_n = g_n(f_n(\text{Response}_n, \text{Claim}_n, \text{State}_{n-1}))\]

\textbf{Notational Convention (Unified):}

\begin{itemize}
\tightlist
\item
  \(K_n\): The full pipeline (embedding + scoring) for layer \(n\)
\item
  \(K^{(n)}\): Shorthand for the same, emphasizing layer-specificity
\item
  \(K(K(x))\): Informal shorthand for \(K_1(x)\), \textbf{not} numerical
  composition
\end{itemize}

This resolves the type ambiguity: \(K\) is \textbf{not} applied to its
own numerical output, but to the embedded representation of a distinct
state object.

\hypertarget{error-model-for-state-functions}{%
\paragraph{Error Model for State
Functions}\label{error-model-for-state-functions}}

\textbf{Deterministic vs Probabilistic Interpretation:}

The functions \(f_n\) and \(g_n\) can be interpreted in two ways:

\textbf{Option A: Deterministic (Default)}

The comparison functions \(f_n\) are deterministic mappings:

\[f_0: \text{Response}(x) \times \text{Reference}(x) \to \{\text{correct}, \text{incorrect}, \text{absent}\}\]

This assumes: - Reference is unambiguous - Response classification is
clear-cut - No measurement error in observation

\textbf{Use case}: Controlled experiments with factual questions and
expert consensus.

\textbf{Option B: Probabilistic (Extended)}

For noisy or uncertain contexts, \(f_n\) can be extended to
probabilistic mappings:

\[f_0: \text{Response}(x) \times \text{Reference}(x) \to \Delta(\{\text{correct}, \text{incorrect}, \text{absent}\})\]

Where \(\Delta(\cdot)\) denotes probability distributions over the
outcome space.

\textbf{Noise Model (if Option B):}

\[P(\text{State}_n = s | \text{True State} = s^*) = \begin{cases}
1 - \epsilon_n & \text{if } s = s^* \\
\epsilon_n / 2 & \text{otherwise}
\end{cases}\]

Where \(\epsilon_n\) is the layer-specific error rate.

\textbf{Recommendation:}

\begin{itemize}
\tightlist
\item
  Use \textbf{deterministic} interpretation for conceptual clarity (this
  paper)
\item
  Use \textbf{probabilistic} extension for empirical work with
  measurement noise
\item
  Always report which interpretation is assumed
\end{itemize}

\hypertarget{entry-and-recursive-mappings}{%
\paragraph{Entry and Recursive
Mappings}\label{entry-and-recursive-mappings}}

\textbf{1. Entry Mapping (Layer 0):}

For an abstract object \(x\) (e.g., a proposition, a task item, or any
epistemic target), the subject's first-order epistemic condition is
represented as a continuous value:

\[k_0(x) \in [-1, 1]\]

This is the \textbf{only point} where the external object \(x\) enters
the model. The internal representation \(k_0\) captures how the subject
stands with respect to \(x\).

\textbf{2. Recursive Mapping (Layers \(n \ge 1\)):}

At all higher layers, the observation family \(\{K^{(n)}\}\) acts on
layer-specific states:

\[K^{(n)}: \mathcal{S}_n \to [-1, 1]\]

The ``object'' of higher-order \(K^{(n)}\) is not the numerical output
of the previous layer but the \textbf{distinct state object}
\(\text{State}_n\).

\textbf{Output Interpretation (Prototypical Anchor Points):}

The values \(-1\), \(0\), and \(1\) serve as \textbf{prototypical
anchors} on the continuous scale \([-1, 1]\):

\begin{itemize}
\tightlist
\item
  \(K(\cdot) = 1\): The subject accurately recognizes the target (full
  knowledge or accurate metacognition).
\item
  \(K(\cdot) = 0\): The subject has no determinate stance regarding the
  target (pure ignorance).
\item
  \(K(\cdot) = -1\): The subject misrecognizes the target (misconception
  or metacognitive failure).
\end{itemize}

All intermediate values represent \textbf{graded mixtures} of these
prototypes (partial knowledge, partial misconception, uncertainty,
etc.).

\textbf{Key Insight:} The function \(K\) has \textbf{consistent
semantics} across all layers: ``How accurately does the subject
recognize this target?'' At Layer 0, the target is an external object
\(x\). At Layers \(n \ge 1\), the target is the subject's own epistemic
state \(k_{n-1}\).

\hypertarget{observation-and-objects-the-purely-observational-framework}{%
\subsubsection{Observation and Objects: The Purely Observational
Framework}\label{observation-and-objects-the-purely-observational-framework}}

This framework adopts a \textbf{purely observational stance}: epistemic
states are operationalized as observable responses, and metacognitive
states as observable alignments between claims and performance. We do
not posit internal ``beliefs'' or ``perceptions'' --- only
\textbf{states} that are measurable by an external observer.

\textbf{The Observer:}

In experimental settings, the ``observer'' is the \textbf{experimenter}
who: 1. Presents a task/question 2. Records the respondent's answer 3.
Compares the answer to a reference 4. Assigns a \(K\) value based on the
comparison

The Observer does not access internal ``perception'' or ``belief.'' The
Observer only sees \textbf{observable behavior}: answers, claims,
responses.

\textbf{Formal Definition of State\(_n\):}

\textbf{State\(_0\) (First-Order Epistemic State):}

\[\text{State}_0(x) = f_0(\text{Response}(x), \text{Reference}(x))\]

Where: - \(\text{Response}(x)\): Subject's answer to item \(x\) -
\(\text{Reference}(x)\): Ground truth or expert consensus - \(f_0\):
Comparison function yielding \{correct, incorrect, absent\}

\textbf{State\(_1\) (Metacognitive State):}

\[\text{State}_1(x) = f_1(\text{Claim}_1(x), \text{State}_0(x))\]

Where: - \(\text{Claim}_1(x)\): Subject's metacognitive claim (``I
know'' / ``I don't know'' / ``I'm wrong'') - \(f_1\): Alignment function
yielding \{aligned, uncertain, misaligned\}

\textbf{State\(_n\) (n-th Order State):}

\[\text{State}_n(x) = f_n(\text{Claim}_n(x), \text{State}_{n-1}(x))\]

\textbf{Graphical Model:}

\begin{verbatim}
Response(x) --+
              +--> f_0 --> State_0 --> K^(0) --> K_0
Reference(x) -+              |
                             v
Claim_1(x) --------------> f_1 --> State_1 --> K^(1) --> K_1
                                    |
                                    v
Claim_2(x) ----------------------> f_2 --> State_2 --> K^(2) --> K_2
\end{verbatim}

\hypertarget{precise-specification-of-state-objects-and-functions}{%
\paragraph{Precise Specification of State Objects and
Functions}\label{precise-specification-of-state-objects-and-functions}}

The following tables provide complete, reproducible definitions for each
layer.

\textbf{State\(_0\): First-Order Epistemic State}

\textbf{Definition:}
\[\text{State}_0 \in \{\text{correct}, \text{incorrect}, \text{absent}\}\]

\textbf{State Function \(f_0\):}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Response \(r\) & Reference \(t\) & \(\text{State}_0 = f_0(r, t)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Any answer \(a\) & \(a = t\) & correct \\
Any answer \(a\) & \(a \neq t\) & incorrect \\
No response / ``I don't know'' & Any & absent \\
\end{longtable}

\textbf{\(K_0\) Computation:} \[K_0(x) = g_0(f_0(r, t))\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{State\(_1\): Second-Order Metacognitive State}

\textbf{Definition:}
\[\text{State}_1 \in \{\text{aligned}, \text{uncertain}, \text{misaligned}\}\]

\textbf{Claim Vocabulary \(C_1\):}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Claim & Meaning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``I know this'' & Subject claims knowledge \\
``I'm not sure'' & Subject expresses uncertainty \\
``I don't know this'' & Subject claims ignorance \\
\end{longtable}

\textbf{State Function \(f_1\):}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1528}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(K_0\) (actual)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Claim\(_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(\text{State}_1 = f_1(K_0, \text{Claim}_1)\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(+1\) (knows) & ``I know'' & aligned \\
\(+1\) (knows) & ``I'm not sure'' & uncertain \\
\(+1\) (knows) & ``I don't know'' & misaligned \\
\(0\) (ignorant) & ``I don't know'' & aligned \\
\(0\) (ignorant) & ``I'm not sure'' & uncertain \\
\(0\) (ignorant) & ``I know'' & misaligned \\
\(-1\) (wrong) & ``I don't know'' & aligned* \\
\(-1\) (wrong) & ``I'm not sure'' & uncertain \\
\(-1\) (wrong) & ``I know'' & misaligned \\
\end{longtable}

*Note: ``I don't know'' when holding a misconception is partial
awareness; coded as ``aligned'' for monotonicity.

\textbf{Rationale: Epistemic Improvement Criterion}

The coding of \(K_0 = -1\) (misconception) + ``I don't know'' → aligned
(\(K_1 = +1\)) may appear counterintuitive. We justify this via the
\textbf{Epistemic Improvement Criterion}: a claim is ``aligned'' if it
represents the best available response given the subject's actual state.

\textbf{Argument}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A subject with \(K_0 = -1\) who claims ``I know'' is \textbf{doubly
  wrong}: wrong about the content AND wrong about their epistemic state.
\item
  A subject with \(K_0 = -1\) who claims ``I don't know'' is
  \textbf{partially correct}: wrong about the content but aware of their
  uncertainty.
\item
  This awareness (\(K_1 = +1\)) is \textbf{epistemically valuable}: it
  opens the door to correction and learning.
\end{enumerate}

\textbf{Best Response for Each \(K_0\)}:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\(K_0\) & Best Claim\(_1\) & Reasoning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(+1\) & ``I know'' & Accurate confidence \\
\(0\) & ``I don't know'' & Accurate ignorance \\
\(-1\) & ``I don't know'' & Protective epistemic humility \\
\end{longtable}

\textbf{Monotonicity Preservation}:

The current coding preserves the ordering:

\[\text{"I know" when wrong} \prec \text{"I don't know" when wrong} \prec \text{"I don't know" when ignorant} \prec \text{"I know" when right}\]

This ordering is monotonic in epistemic quality and consistent with the
Socratic wisdom tradition.

\textbf{Alternative: Graded Alignment (Future Extension)}

A graded scheme could assign: - \(K_0 = -1\), Claim = ``I don't know'' →
\(K_1 = 0.5\) (partial alignment) - \(K_0 = -1\), Claim = ``I know'' →
\(K_1 = -1\) (full misalignment)

This is a valid design choice but complicates anchor semantics. We leave
graded alignment for future extension.

\hypertarget{decision-theoretic-analysis-of-coding-choices}{%
\paragraph{Decision-Theoretic Analysis of Coding
Choices}\label{decision-theoretic-analysis-of-coding-choices}}

The coding of \(K_0 = -1\) (misconception) + ``I don't know'' → aligned
(\(K_1 = +1\)) has been justified via the Epistemic Improvement
Criterion above. Here we provide a complementary
\textbf{decision-theoretic} analysis.

\hypertarget{the-contested-case-revisited}{%
\subparagraph{The Contested Case
Revisited}\label{the-contested-case-revisited}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1489}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2340}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3404}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2766}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(K_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Claim\(_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Current Coding
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alternative
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(-1\) (misconception) & ``I don't know'' & aligned (\(K_1 = +1\)) &
partial (\(K_1 = 0\))? \\
\end{longtable}

\hypertarget{decision-theoretic-framework}{%
\subparagraph{Decision-Theoretic
Framework}\label{decision-theoretic-framework}}

Define a loss function \(L(K_0, \text{Claim}_1, \text{Action})\) where
Action is taken based on the claim.

\textbf{Scenario: Selective Prediction}

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
\(K_0\) & Claim\(_1\) & Action & Outcome & Loss \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(-1\) & ``I know'' & Trust answer & Wrong answer used & High \\
\(-1\) & ``I don't know'' & Abstain & Correct abstention & Low \\
\(+1\) & ``I know'' & Trust answer & Correct answer used & Low \\
\(+1\) & ``I don't know'' & Abstain & Missed opportunity & Medium \\
\end{longtable}

\textbf{Implication}: Under selective prediction loss, ``I don't know''
when \(K_0 = -1\) is \emph{optimal}, supporting \(K_1 = +1\) coding.

\hypertarget{when-current-coding-may-be-suboptimal}{%
\subparagraph{When Current Coding May Be
Suboptimal}\label{when-current-coding-may-be-suboptimal}}

\textbf{Scenario: Forced Response}

If abstention is not allowed: - ``I don't know'' when \(K_0 = -1\) does
not prevent the wrong answer from being used - The coding \(K_1 = +1\)
may overstate alignment

\textbf{Scenario: Asymmetric Costs}

If false positives are much worse than false negatives: - ``I don't
know'' provides less protection than explicit correction - A more
conservative coding (\(K_1 = 0\)) might be appropriate

\hypertarget{coding-sensitivity-analysis}{%
\subparagraph{Coding Sensitivity
Analysis}\label{coding-sensitivity-analysis}}

We present the current coding as the \textbf{default} under the
following assumptions: 1. Abstention is possible 2. Costs are
approximately symmetric 3. Epistemic improvement (openness to
correction) is valued

\textbf{Alternative Coding Table} (for specialized applications):

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
\(K_0\) & Claim\(_1\) & Default \(K_1\) & Conservative \(K_1\) &
Rationale \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(-1\) & ``I know'' & \(-1\) & \(-1\) & Unanimous: worst case \\
\(-1\) & ``I don't know'' & \(+1\) & \(0\) & Contested: depends on
loss \\
\(-1\) & ``Not sure'' & \(0\) & \(0\) & Appropriate uncertainty \\
\end{longtable}

\textbf{Recommendation}: Use default coding unless task-specific loss
analysis indicates otherwise. Document coding choice and rationale.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{State\(_2\): Third-Order Meta-Metacognitive State}

\textbf{Definition:}
\[\text{State}_2 \in \{\text{meta-aligned}, \text{meta-uncertain}, \text{meta-misaligned}\}\]

\textbf{Claim Vocabulary \(C_2\):}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4375}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5625}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Claim
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``My self-assessment is accurate'' & Subject endorses their \(K_1\) \\
``I'm not sure about my self-assessment'' & Subject uncertain about
\(K_1\) \\
``My self-assessment may be wrong'' & Subject doubts their \(K_1\) \\
\end{longtable}

\textbf{State Function \(f_2\) (Complete 9-Pattern Enumeration):}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3556}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2444}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2444}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1556}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(K_1\) (actual)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Claim\(_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
State\(_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_2\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(+1\) (aligned) & ``My self-assessment is accurate'' & meta-aligned &
\(+1\) \\
\(+1\) (aligned) & ``I'm not sure about my self-assessment'' &
meta-uncertain & \(0\) \\
\(+1\) (aligned) & ``My self-assessment may be wrong'' & meta-misaligned
& \(-1\) \\
\(0\) (uncertain) & ``My self-assessment is accurate'' & meta-misaligned
& \(-1\) \\
\(0\) (uncertain) & ``I'm not sure about my self-assessment'' &
meta-aligned & \(+1\) \\
\(0\) (uncertain) & ``My self-assessment may be wrong'' &
meta-misaligned & \(-1\) \\
\(-1\) (misaligned) & ``My self-assessment is accurate'' &
meta-misaligned & \(-1\) \\
\(-1\) (misaligned) & ``I'm not sure about my self-assessment'' &
meta-uncertain & \(0\) \\
\(-1\) (misaligned) & ``My self-assessment may be wrong'' & meta-aligned
& \(+1\) \\
\end{longtable}

\textbf{Interpretation of State\(_2\) Values:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4074}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2593}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
State\(_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{meta-aligned} & \(+1\) & Subject's belief about their
self-assessment accuracy matches reality \\
\textbf{meta-uncertain} & \(0\) & Subject expresses uncertainty about
their self-assessment \\
\textbf{meta-misaligned} & \(-1\) & Subject's belief about their
self-assessment accuracy contradicts reality \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Summary: The Complete Pipeline}

\begin{verbatim}
Response(x) -> f$_0$(Response, Reference) -> State$_0$ -> g$_0$ -> K$_0$
                                          v
Claim$_1$ + K$_0$ -> f$_1$(K$_0$, Claim$_1$) -> State$_1$ -> g$_1$ -> K$_1$
                                          v
Claim$_2$ + K$_1$ -> f$_2$(K$_1$, Claim$_2$) -> State$_2$ -> g$_2$ -> K$_2$
\end{verbatim}

\textbf{Reproducibility:} Given the same (Response, Claim\(_1\),
Claim\(_2\), Reference), any observer following this specification will
compute identical \((K_0, K_1, K_2)\).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{quote}
\textbf{DEFINITION (Higher-Order Alignment States):}

For layer \(n \geq 1\), the alignment state is determined by comparing
actual \(K_{n-1}\) with Claim\(_n\):

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Term & Condition & \(K_n\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Aligned} & Claim\(_n\) correctly describes \(K_{n-1}\) &
\(+1\) \\
\textbf{Uncertain} & Claim\(_n\) = ``I'm not sure'' AND \(K_{n-1} = 0\)
& \(0\) \\
\textbf{Misaligned} & Claim\(_n\) contradicts \(K_{n-1}\) & \(-1\) \\
\end{longtable}

\textbf{Formal Rule}:
\[K_n = \begin{cases} +1 & \text{if Claim}_n \text{ matches } K_{n-1} \\ 0 & \text{if Claim}_n = \text{"uncertain"} \text{ and } K_{n-1} = 0 \\ -1 & \text{if Claim}_n \text{ contradicts } K_{n-1} \end{cases}\]
\end{quote}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Operational Interpretation:}

\begin{itemize}
\tightlist
\item
  \textbf{State\(_0\) (first-order epistemic state)}: The respondent's
  answer compared to a reference.

  \begin{itemize}
  \tightlist
  \item
    Operationalized as: ``Is the answer correct, incorrect, or absent?''
  \end{itemize}
\item
  \textbf{State\(_1\) (metacognitive state)}: The alignment between the
  respondent's metacognitive claim and their actual State\(_0\).

  \begin{itemize}
  \tightlist
  \item
    Operationalized as: ``Does the claim `I know' match the actual
    correctness?''
  \end{itemize}
\item
  \textbf{State\(_2\) (meta-metacognitive state)}: The alignment between
  the respondent's meta-metacognitive claim and their actual
  State\(_1\).
\end{itemize}

\textbf{Example:} - Respondent answers incorrectly -\textgreater{}
State\(_0\) = ``incorrect'' -\textgreater{} \(K_0 = -1\) (misconception)
- Respondent says ``I don't know'' -\textgreater{} State\(_0\) =
``absent'' -\textgreater{} \(K_0 = 0\) (ignorance) - Respondent claims
``I know'' (when \(K_0 = -1\)) -\textgreater{} State\(_1\) =
``misalignment'' -\textgreater{} \(K_1 = -1\) - Respondent claims ``My
self-assessment is accurate'' -\textgreater{} State\(_2\) =
``misalignment'' -\textgreater{} \(K_2 = -1\)

\textbf{Critical Clarification:}

Each \(K_n\) is an \textbf{independent observation} of a
\textbf{distinct object} (State\(_n\)):

\begin{itemize}
\tightlist
\item
  \textbf{\(K_0(x)\)}: Observer's measurement of \textbf{State\(_0\)}
\item
  \textbf{\(K_1(x)\)}: Observer's measurement of \textbf{State\(_1\)}
\item
  \textbf{\(K_2(x)\)}: Observer's measurement of \textbf{State\(_2\)}
\end{itemize}

\[K_1(x) \neq K(K_0(x))\]

\(K_1\) is \textbf{not} ``applying \(K\) to the numerical value of
\(K_0\).'' \(K_1\) is ``observing a different object (State\(_1\)) and
reporting the measurement.''

\textbf{Resolving the Apparent Contradiction (\(K(0) = 0\) vs
\(K_1 = -1\)):}

The axiom \(K(0) = 0\) means: ``If the observed state is `ignorance'
(0), the measurement result is `ignorance' (0).''

In the Dunning-Kruger case: - \(K_0(x) = 0\): Observer measures
State\(_0\) -\textgreater{} ``ignorance'' - \(K_1(x) = -1\): Observer
measures State\(_1\) -\textgreater{} ``misalignment''

\textbf{State\(_1\) is not ``0''.} State\(_1\) is the metacognitive
state (alignment/misalignment), which the observer measures as
``misrecognition'' (-1).

The axiom \(K(0) = 0\) does not apply because the input to \(K_1\) is
\textbf{not} the number ``0''. The input is \textbf{State\(_1\)}, a
different object entirely.

\textbf{Analogy (Thermometer Calibration):}

Consider a thermometer and its calibration: - \textbf{State\(_0\)
(temperature)}: The actual temperature of water = 20\(^\circ\)C -
\textbf{State\(_1\) (thermometer accuracy)}: Whether the thermometer
correctly reads State\(_0\)

Measuring State\(_0\) = 20\(^\circ\)C does not constrain State\(_1\).
The thermometer might be: - Accurate (State\(_1\) = correct)
-\textgreater{} \(K_1 = 1\) - Miscalibrated (State\(_1\) = incorrect)
-\textgreater{} \(K_1 = -1\)

\(K_1\) measures a \textbf{property of the measuring instrument}, not
the original object. Similarly, \(K_1\) measures the accuracy of
\textbf{the respondent's self-monitoring}, not the first-order state
itself.

\hypertarget{observation-vs-intervention-clarifying-the-frameworks-scope}{%
\paragraph{Observation vs Intervention: Clarifying the Framework's
Scope}\label{observation-vs-intervention-clarifying-the-frameworks-scope}}

\textbf{The Observational Stance:}

This framework is \textbf{observational} in the following sense: -
\(K_n\) values are determined by observable behavior (responses, claims)
- No internal mental states are posited - The observer does not require
privileged access to the subject's mind

\textbf{The Interventional Possibility:}

The phrase ``targeted intervention'' in the Introduction refers to a
\textbf{downstream application}, not a claim within the framework
itself:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Observation}: Measure \(K_0\), \(K_1\), \(K_2\) via the MAT
  protocol
\item
  \textbf{Classification}: Identify metacognitive pattern (e.g.,
  Dunning-Kruger: \(K_0=0\), \(K_1=-1\))
\item
  \textbf{Intervention design} (external to framework): Choose
  intervention based on classification
\item
  \textbf{Re-observation}: Measure \(K_n\) again to assess intervention
  effect
\end{enumerate}

\textbf{What the Framework Provides:}

\begin{itemize}
\tightlist
\item
  \textbf{Coordinates} for locating epistemic states
\item
  \textbf{Taxonomy} for classifying metacognitive patterns
\item
  \textbf{Outcome measures} for evaluating interventions
\end{itemize}

\textbf{What the Framework Does NOT Provide:}

\begin{itemize}
\tightlist
\item
  \textbf{Causal model} of how interventions change \(K_n\)
\item
  \textbf{Mechanism} by which metacognition operates
\item
  \textbf{Prescriptions} for which interventions to use
\end{itemize}

\emph{See Thermometer Calibration Analogy above for an illustration of
the observation/intervention distinction.}

\hypertarget{symbolic-notation-a-conceptual-communication-tool}{%
\subsubsection{Symbolic Notation: A Conceptual Communication
Tool}\label{symbolic-notation-a-conceptual-communication-tool}}

\textbf{Purpose and Status:}

The notation \(K(K(x))\) serves as an \textbf{intuitive communication
device} for conveying the core insight of recursive metacognition to
interdisciplinary audiences. It captures the philosophical essence of
``not knowing that one does not know'' more vividly than indexed
notation.

\textbf{Formal Status:} - This notation is \textbf{NOT used in formal
definitions, proofs, or measurement protocols} - All formal operations
are defined via \(K_n(\text{State}_n)\) exclusively - The symbolic
notation has \textbf{NO independent operational semantics}

\textbf{Translation Convention:}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Symbolic (intuitive) & Formal (operational) & Meaning \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K(x)\) & \(K_0(x)\) & First-order epistemic observation \\
\(K(K(x))\) & \(K_1(x)\) & Second-order metacognitive observation \\
\(K(K(K(x)))\) & \(K_2(x)\) & Third-order meta-metacognitive
observation \\
\end{longtable}

\textbf{Why Retain Symbolic Notation?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Philosophical resonance}: The recursive imagery
  \(K(K(K(...)))\) conveys the unbounded nature of self-reflection
\item
  \textbf{Interdisciplinary accessibility}: Non-technical readers grasp
  the recursive structure intuitively
\item
  \textbf{Historical continuity}: Echoes classical formulations
  (Socrates' ``knowing that I do not know'')
\end{enumerate}

\textbf{What Symbolic Notation Does NOT Provide:} - Mathematical type
structure - Operational definitions - Measurement protocols - Formal
proofs

\textbf{The \(K_n\) notation is the SOLE formal apparatus of this
framework.}

\hypertarget{axiomatic-constraints-on-k}{%
\subsubsection{\texorpdfstring{Axiomatic Constraints on
\(K\)}{Axiomatic Constraints on K}}\label{axiomatic-constraints-on-k}}

We impose the following minimal constraints on the epistemic function
\(K\):

\textbf{Definition: Objective Evaluation}

The function \(K\) represents an \textbf{objective evaluation} of the
subject's epistemic state by an external observer (or the system),
distinct from the subject's subjective feeling of confidence (\(C\)).

\begin{itemize}
\tightlist
\item
  \(K(x) = 1\): Objectively accurate recognition.
\item
  \(K(x) = -1\): Objectively inverted recognition (misconception).
\end{itemize}

\textbf{Axiom Scope:}

The axioms describe the behavior of the observation function \(K\)
\textbf{within a single layer}.

\begin{itemize}
\tightlist
\item
  \(K(1) = 1\): If the observed state is ``knowledge'', report
  ``knowledge''
\item
  \(K(0) = 0\): If the observed state is ``ignorance'', report
  ``ignorance''
\item
  \(K(-1) = -1\): If the observed state is ``misconception'', report
  ``misconception''
\end{itemize}

\textbf{Layer Independence:}

Each \(K_n\) observes a \textbf{different object} (State\(_n\)). The
axioms apply to each observation independently.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3846}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3077}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3077}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Object Observed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Axiom Application
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & State\(_0\) (first-order epistemic state) & \(K\)(State\(_0\))
follows axioms \\
\(K_1\) & State\(_1\) (metacognitive state) & \(K\)(State\(_1\)) follows
axioms \\
\(K_2\) & State\(_2\) (meta-metacognitive state) & \(K\)(State\(_2\))
follows axioms \\
\end{longtable}

\textbf{Why \(K_0 = 0\) and \(K_1 = -1\) is NOT a contradiction:}

\begin{itemize}
\tightlist
\item
  \(K_0 = 0\): Observer measures State\(_0\) as ``ignorance''
\item
  \(K_1 = -1\): Observer measures State\(_1\) as ``misrecognition''
\end{itemize}

State\(_0 \neq\) State\(_1\). They are different objects. The axiom
\(K(0) = 0\) applies to State\(_0\), not to State\(_1\).

\hypertarget{layer-independence-formal-conditions}{%
\paragraph{Layer Independence: Formal
Conditions}\label{layer-independence-formal-conditions}}

\hypertarget{conditional-independence-assumption}{%
\subparagraph{Conditional Independence
Assumption}\label{conditional-independence-assumption}}

\textbf{Definition (Layer Separation)}:

\[\text{State}_n \perp\!\!\!\perp \text{State}_{n-2} \mid \text{State}_{n-1}\]

This states that \(\text{State}_n\) (the object of layer-\(n\)
assessment) depends only on \(\text{State}_{n-1}\), not on earlier
layers.

\textbf{Implication for Scoring}:

\[K_n = f_n(\text{Response}_n, \text{Claim}_n, \text{State}_{n-1})\]

The scoring function \(f_n\) does not require access to
\(\text{State}_{n-2}\) or earlier.

\hypertarget{no-circular-dependency}{%
\subparagraph{No Circular Dependency}\label{no-circular-dependency}}

\textbf{Definition (Acyclicity)}:

The dependency graph over \(\{K_0, K_1, K_2, \ldots\}\) is a directed
acyclic graph (DAG):

\[K_0 \to K_1 \to K_2 \to \cdots\]

Where \(K_n\) depends on \(K_{n-1}\) (via \(\text{State}_{n-1}\)) but
not vice versa.

\textbf{Proof of Acyclicity}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(K_0\) is computed from (Response, Reference) alone---no dependency
  on higher layers
\item
  \(K_1\) is computed from (Claim\(_1\), \(K_0\))---depends only on
  \(K_0\)
\item
  \(K_n\) is computed from (Claim\(_n\), \(K_{n-1}\))---depends only on
  \(K_{n-1}\)
\end{enumerate}

By induction, no backward dependencies exist. \(\square\)

\hypertarget{graphical-model-representation}{%
\subparagraph{Graphical Model
Representation}\label{graphical-model-representation}}

The dependency structure can be visualized as a cascade:

\[\text{Reference} \to \text{State}_0 \to K_0 \to \text{State}_1 \to K_1 \to \text{State}_2 \to K_2 \to \cdots\]

Each layer is a separate ``plate'' in the graphical model, with
information flowing strictly forward (lower to higher layers).

\hypertarget{identifiability-conditions}{%
\subparagraph{Identifiability
Conditions}\label{identifiability-conditions}}

\textbf{Definition (\(K_n\) Identifiability)}:

\(K_n\) is \textbf{identifiable} if, given sufficient observations of
(Response, Claim\(_1\), \ldots, Claim\(_n\), Reference), the value of
\(K_n\) can be uniquely determined.

\textbf{Sufficient Conditions}:

\textbf{Condition 1 (Reference Availability)}:

The reference (ground truth) for \(\text{State}_0\) must be available or
reliably estimable.

\[\exists \text{ Reference}: P(\text{Reference} = \text{true state}) = 1\]

Or, under probabilistic scoring:

\[P(\text{Reference} | \text{evidence}) \text{ is well-defined}\]

\textbf{Condition 2 (Claim Observability)}:

Claims at each layer must be observable:

\[\text{Claim}_n \in \mathcal{C}_n \text{ is directly elicited and recorded}\]

\textbf{Condition 3 (Deterministic Mapping)}:

Under Option A (deterministic scoring):

\[f_n: \mathcal{C}_n \times \{-1, 0, +1\} \to \{-1, 0, +1\}\]

is a fixed, known function. Given (Claim\(_n\), \(K_{n-1}\)), \(K_n\) is
uniquely determined.

\textbf{Condition 4 (Probabilistic Identifiability)}:

Under Option B (probabilistic scoring), identifiability requires:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Sufficient variation}: Items vary in true \(K_0\) values
\item
  \textbf{Informative claims}: \(P(\text{Claim}_1 | K_0)\) differs
  across \(K_0\) values
\item
  \textbf{No confounding}: Claim\(_n\) depends on \(K_{n-1}\) but not on
  unobserved variables
\end{enumerate}

\textbf{Identifiability Violations}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3939}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Violation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Consequence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reference unavailable & Contested scientific claims & \(K_0\)
undefined \\
Claims unobserved & Internal confidence only & \(K_1\) inestimable \\
Deterministic degeneracy & All subjects say ``I know'' & \(K_1\)
variance = 0 \\
Confounding & Claim influenced by social desirability & Biased
\(K_1\) \\
\end{longtable}

\textbf{Practical Recommendation}:

For robust estimation: 1. Use items with known references (e.g., factual
questions with verified answers) 2. Elicit claims explicitly (not
inferred from behavior) 3. Include items spanning
\(K_0 \in \{-1, 0, +1\}\) to ensure variation 4. Control for demand
characteristics via randomized claim formats

\textbf{Monotonicity (Revised):}

\textbf{Definition (Order on Embedded States):}

For embedded values \(k, k' \in [-1, 1]\), the natural order \(k > k'\)
applies.

\textbf{Monotonicity Axiom:}

For any scoring function \(\hat{K}\):

\[\text{If } g_n(\text{State}_n) > g_n(\text{State}'_n), \text{ then } K_n \geq K'_n\]

This is trivially satisfied when \(\hat{K}\) is the identity on anchors
and monotonic elsewhere.

\textbf{Practical Interpretation:}

Monotonicity ensures that ``more aligned'' states receive higher \(K\)
values. This does not constrain the shape of \(\hat{K}\) beyond anchor
preservation.

Monotonicity applies \textbf{within each layer} only. It does NOT
constrain relationships \textbf{across layers} (e.g., \(K_0\) vs
\(K_1\)).

\textbf{Boundedness:}

\(K: [-1, 1] \to [-1, 1]\)

Each observation is bounded on this interval.

\textbf{Note:} We deliberately refrain from specifying stronger
constraints (e.g., odd symmetry, Lipschitz constant, contraction
mapping) at this stage. The framework is intended to be
\textbf{descriptive} rather than \textbf{predictive}---it provides a
vocabulary for classifying observed metacognitive states, not a
generative model of metacognitive dynamics. Specifying a particular
functional form for \(K\) is a task for domain-specific empirical
research.

\hypertarget{recursive-application}{%
\subsubsection{Recursive Application}\label{recursive-application}}

\textbf{Notation:}

We use subscript notation to denote the layer of observation:

\[K_0(x) = K(\text{State}_0)\] \[K_1(x) = K(\text{State}_1)\]
\[K_2(x) = K(\text{State}_2)\]

\textbf{Important Clarification:}

The traditional notation \(K(K(x))\) is \textbf{shorthand} for
\(K_1(x)\), but it should \textbf{not} be interpreted as function
composition (i.e., \(K(K_0(x))\) where the numerical value of \(K_0\) is
passed to \(K\)).

Each \(K_n\) observes a \textbf{distinct object} (State\(_n\)), not the
numerical output of the previous layer.

\textbf{Interpretation:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expression
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Object Observed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Question
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Layer 0} & \(K_0(x)\) & State\(_0\) (first-order epistemic
state) & ``Is the respondent's answer correct?'' \\
\textbf{Layer 1} & \(K_1(x)\) & State\(_1\) (metacognitive state) & ``Is
the respondent's self-assessment aligned with their actual
State\(_0\)?'' \\
\textbf{Layer 2} & \(K_2(x)\) & State\(_2\) (meta-metacognitive state) &
``Is the respondent's meta-self-assessment aligned with their actual
State\(_1\)?'' \\
\end{longtable}

\textbf{Same observation function \(K\). Different objects
(State\(_n\)). Independent measurements.}

\hypertarget{examples}{%
\subsubsection{Examples}\label{examples}}

\hypertarget{example-1-knowing-knowledge}{%
\paragraph{Example 1: Knowing
Knowledge}\label{example-1-knowing-knowledge}}

\begin{itemize}
\tightlist
\item
  \(K(x) = 1\): The subject knows that ``water boils at
  100\(^\circ\)C.''
\item
  \(K(K(x)) = 1\): The subject accurately recognizes that they know this
  fact.
\item
  \textbf{Classification}: Knowing Knowledge (accurate self-awareness)
\end{itemize}

\hypertarget{example-2-socratic-wisdom}{%
\paragraph{Example 2: Socratic Wisdom}\label{example-2-socratic-wisdom}}

\begin{itemize}
\tightlist
\item
  \(K(x) = 0\): The subject does not know the boiling point of water.
\item
  \(K(K(x)) = 1\): The subject accurately recognizes their ignorance
  (``I know that I don't know'').
\item
  \textbf{Classification}: Knowing Ignorance (Socratic wisdom)
\end{itemize}

\hypertarget{example-3-dunning-kruger-effect}{%
\paragraph{Example 3: Dunning-Kruger
Effect}\label{example-3-dunning-kruger-effect}}

\begin{itemize}
\tightlist
\item
  \(K(x) = 0\): The subject does not know the boiling point of water.
\item
  \(K(K(x)) = -1\): The subject misrecognizes their ignorance, believing
  they know.
\item
  \textbf{Classification}: Unknowing Ignorance (Dunning-Kruger effect)
\end{itemize}

\hypertarget{example-4-imposter-syndrome}{%
\paragraph{Example 4: Imposter
Syndrome}\label{example-4-imposter-syndrome}}

\begin{itemize}
\tightlist
\item
  \(K(x) = 1\): The subject knows that ``water boils at
  100\(^\circ\)C.''
\item
  \(K(K(x)) = -1\) or \(0\): The subject does not recognize their
  knowledge (``I don't think I know this'').
\item
  \textbf{Classification}: Unknowing Knowledge (imposter syndrome)
\end{itemize}

\hypertarget{the-four-quadrants-of-metacognition}{%
\subsubsection{The Four Quadrants of
Metacognition}\label{the-four-quadrants-of-metacognition}}

The relationship between \(K(x)\) (actual state) and \(K(K(x))\)
(metacognitive accuracy) produces four archetypal patterns:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2778}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
\(K(x)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(K(K(x))\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) (Know) & \(1\) (Accurate) & \textbf{Knowing Knowledge} & Accurate
self-awareness \\
\(0\) (Ignorant) & \(1\) (Accurate) & \textbf{Knowing Ignorance} &
Socratic wisdom \\
\(0\) (Ignorant) & \(-1\) (Misrecognition) & \textbf{Unknowing
Ignorance} & Dunning-Kruger effect \\
\(1\) (Know) & \(-1\) or \(0\) & \textbf{Unknowing Knowledge} & Imposter
syndrome \\
\end{longtable}

\textbf{Important Note:} The value \(K(K(x)) = -1\) for ``Unknowing
Ignorance'' does \textbf{not} mean it is ``bad'' in a normative sense.
It simply describes an epistemic state where the subject
\textbf{misrecognizes their own ignorance}. Whether this is problematic
depends on context and goals.

\hypertarget{complete-taxonomy-k_0-times-k_1-times-k_2-27-patterns}{%
\subsubsection{\texorpdfstring{Complete Taxonomy:
\(K_0 \times K_1 \times K_2\) (27
Patterns)}{Complete Taxonomy: K\_0 \textbackslash times K\_1 \textbackslash times K\_2 (27 Patterns)}}\label{complete-taxonomy-k_0-times-k_1-times-k_2-27-patterns}}

\textbf{Derivation:}

Each of \(K_0\), \(K_1\), \(K_2\) takes values in \(\{-1, 0, +1\}\)
(discretized from continuous scale via thresholds). Total
configurations: \(3 \times 3 \times 3 = 27\)

\textbf{The Full Table:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0698}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1628}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1628}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1628}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1395}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3023}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\#
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & +1 & +1 & +1 & \textbf{Perfect Calibration} & Knows, knows they
know, knows they know they know \\
2 & +1 & +1 & 0 & \textbf{Unreflective Expert} & Knows and knows it, but
unaware of this meta-state \\
3 & +1 & +1 & -1 & \textbf{Doubting Expert} & Knows and knows it, but
believes their self-knowledge is poor \\
4 & +1 & 0 & +1 & \textbf{Agnostic Knower (Meta-aware)} & Knows but
unsure if they know, aware of this uncertainty \\
5 & +1 & 0 & 0 & \textbf{Agnostic Knower} & Knows but unsure if they
know \\
6 & +1 & 0 & -1 & \textbf{Falsely Uncertain} & Knows but unsure,
believes wrongly they're certain \\
7 & +1 & -1 & +1 & \textbf{Imposter (Self-aware)} & Knows but thinks
they don't, aware of this pattern \\
8 & +1 & -1 & 0 & \textbf{Classic Imposter} & Knows but thinks they
don't know \\
9 & +1 & -1 & -1 & \textbf{Deep Imposter} & Knows, thinks they don't,
believes their self-doubt is accurate \\
10 & 0 & +1 & +1 & \textbf{Aware Ignorance (Validated)} & Doesn't know,
knows this, confident in this self-knowledge \\
11 & 0 & +1 & 0 & \textbf{Socratic Wisdom} & Doesn't know and knows it
(``I know that I know nothing'') \\
12 & 0 & +1 & -1 & \textbf{Doubting Socrates} & Doesn't know, knows it,
but doubts this self-knowledge \\
13 & 0 & 0 & +1 & \textbf{Pure Uncertainty (Meta-aware)} & Doesn't know,
unsure if they know, aware of confusion \\
14 & 0 & 0 & 0 & \textbf{Complete Uncertainty} & Doesn't know, unsure if
they know, unsure about that \\
15 & 0 & 0 & -1 & \textbf{Confused Certainty} & Doesn't know, unsure,
but believes they're clear \\
16 & 0 & -1 & +1 & \textbf{Dunning-Kruger (Self-aware)} & Doesn't know,
thinks they know, aware of this bias \\
17 & 0 & -1 & 0 & \textbf{Classic Dunning-Kruger} & Doesn't know but
thinks they know \\
18 & 0 & -1 & -1 & \textbf{Deep Dunning-Kruger} & Doesn't know, thinks
they know, confident in false belief \\
19 & -1 & +1 & +1 & \textbf{Aware Misconception (Validated)} & Has
misconception, knows it, confident in this \\
20 & -1 & +1 & 0 & \textbf{Aware Misconception} & Has misconception and
knows it \\
21 & -1 & +1 & -1 & \textbf{Doubting Awareness} & Has misconception,
knows it, but doubts this knowledge \\
22 & -1 & 0 & +1 & \textbf{Uncertain Misconception (Meta-aware)} & Has
misconception, unsure, aware of uncertainty \\
23 & -1 & 0 & 0 & \textbf{Uncertain Misconception} & Has misconception,
unsure if they know \\
24 & -1 & 0 & -1 & \textbf{Falsely Certain Misconception} & Has
misconception, unsure, but thinks they're sure \\
25 & -1 & -1 & +1 & \textbf{Confident Error (Self-aware)} & Has
misconception, thinks it's knowledge, aware of this risk \\
26 & -1 & -1 & 0 & \textbf{Confident Error} & Has misconception and
thinks it's knowledge \\
27 & -1 & -1 & -1 & \textbf{Entrenched Error} & Has misconception,
thinks it's knowledge, certain of this \\
\end{longtable}

\textbf{Completeness Argument:}

The taxonomy is complete by construction: every possible
\((K_0, K_1, K_2) \in \{-1, 0, +1\}^3\) combination is enumerated. No
configuration is possible outside this space under the discretized
model.

\textbf{Notable Patterns:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1525}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2542}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5932}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Configuration
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Clinical/Educational Significance
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Socratic Wisdom} & \((0, +1, \cdot)\) & Ideal starting point for
learning \\
\textbf{Classic Dunning-Kruger} & \((0, -1, 0)\) & Intervention target:
calibration training \\
\textbf{Classic Imposter} & \((+1, -1, 0)\) & Intervention target:
confidence building \\
\textbf{Entrenched Error} & \((-1, -1, -1)\) & Most resistant to change;
requires staged approach \\
\textbf{Perfect Calibration} & \((+1, +1, +1)\) & Ideal end state \\
\end{longtable}

\hypertarget{worked-examples-representative-patterns}{%
\subsubsection{Worked Examples: Representative
Patterns}\label{worked-examples-representative-patterns}}

The following examples demonstrate how the \((K_0, K_1, K_2)\) triplet
is computed in concrete scenarios.

\hypertarget{example-1-socratic-wisdom-pattern-11-k_0-0-k_1-1-k_2-0}{%
\paragraph{\texorpdfstring{Example 1: Socratic Wisdom (Pattern \#11:
\(K_0 = 0, K_1 = +1, K_2 = 0\))}{Example 1: Socratic Wisdom (Pattern \#11: K\_0 = 0, K\_1 = +1, K\_2 = 0)}}\label{example-1-socratic-wisdom-pattern-11-k_0-0-k_1-1-k_2-0}}

\textbf{Scenario}: History exam, question about the date of the Treaty
of Westphalia.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1842}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Observable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Computation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Value
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Response & ``I don't know'' & --- & --- \\
Reference & 1648 & --- & --- \\
State\(_0\) & Response = Absent & \(f_0(\text{absent}, 1648)\) = absent
& --- \\
\(K_0\) & \(g_0(\text{absent})\) & \(= 0\) & \textbf{\(K_0 = 0\)} \\
Claim\(_1\) & ``I correctly identified that I don't know'' & --- &
--- \\
State\(_1\) & Claim\(_1\) matches \(K_0 = 0\) &
\(f_1(0, \text{"I don't know"})\) = aligned & --- \\
\(K_1\) & \(g_1(\text{aligned})\) & \(= +1\) & \textbf{\(K_1 = +1\)} \\
Claim\(_2\) & ``I'm not sure about my self-assessment'' & --- & --- \\
State\(_2\) & Claim\(_2\) = uncertain when \(K_1 = +1\) &
\(f_2(+1, \text{"not sure"})\) = uncertain & --- \\
\(K_2\) & \(g_2(\text{uncertain})\) & \(= 0\) & \textbf{\(K_2 = 0\)} \\
\end{longtable}

\textbf{Interpretation}: Subject demonstrates Socratic wisdom---accurate
recognition of their own ignorance---but is modest about this
metacognitive achievement.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-2-deep-dunning-kruger-pattern-18-k_0-0-k_1--1-k_2--1}{%
\paragraph{\texorpdfstring{Example 2: Deep Dunning-Kruger (Pattern \#18:
\(K_0 = 0, K_1 = -1, K_2 = -1\))}{Example 2: Deep Dunning-Kruger (Pattern \#18: K\_0 = 0, K\_1 = -1, K\_2 = -1)}}\label{example-2-deep-dunning-kruger-pattern-18-k_0-0-k_1--1-k_2--1}}

\textbf{Scenario}: Math test, question ``What is \(\sqrt{16}\)?''

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1842}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Observable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Computation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Value
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Response & ``5'' & --- & --- \\
Reference & 4 & --- & --- \\
State\(_0\) & Response \(\neq\) Reference & \(f_0(5, 4)\) = incorrect &
--- \\
\(K_0\) & \(g_0(\text{incorrect})\) & \(= -1\) &
\textbf{\(K_0 = -1\)} \\
Claim\(_1\) & ``I'm confident I'm correct'' & --- & --- \\
State\(_1\) & Claim\(_1\) contradicts \(K_0 = -1\) &
\(f_1(-1, \text{"I know"})\) = misaligned & --- \\
\(K_1\) & \(g_1(\text{misaligned})\) & \(= -1\) &
\textbf{\(K_1 = -1\)} \\
Claim\(_2\) & ``My self-assessment is reliable'' & --- & --- \\
State\(_2\) & Claim\(_2\) contradicts \(K_1 = -1\) &
\(f_2(-1, \text{"accurate"})\) = meta-misaligned & --- \\
\(K_2\) & \(g_2(\text{meta-misaligned})\) & \(= -1\) &
\textbf{\(K_2 = -1\)} \\
\end{longtable}

\textbf{Interpretation}: Triple misalignment---wrong answer,
overconfident, and unaware of overconfidence. This is the ``entrenched''
metacognitive failure pattern.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{example-3-imposter-syndrome-aware-pattern-7-k_0-1-k_1--1-k_2-1}{%
\paragraph{\texorpdfstring{Example 3: Imposter Syndrome Aware (Pattern
\#7:
\(K_0 = +1, K_1 = -1, K_2 = +1\))}{Example 3: Imposter Syndrome Aware (Pattern \#7: K\_0 = +1, K\_1 = -1, K\_2 = +1)}}\label{example-3-imposter-syndrome-aware-pattern-7-k_0-1-k_1--1-k_2-1}}

\textbf{Scenario}: Programming task, correct solution submitted.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1579}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3158}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3421}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1842}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Step
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Observable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Computation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Value
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Response & Correct code & --- & --- \\
Reference & Expected output & --- & --- \\
State\(_0\) & Response = Reference &
\(f_0(\text{correct}, \text{correct})\) = correct & --- \\
\(K_0\) & \(g_0(\text{correct})\) & \(= +1\) & \textbf{\(K_0 = +1\)} \\
Claim\(_1\) & ``I probably got it wrong'' & --- & --- \\
State\(_1\) & Claim\(_1\) contradicts \(K_0 = +1\) &
\(f_1(+1, \text{"I don't know"})\) = misaligned & --- \\
\(K_1\) & \(g_1(\text{misaligned})\) & \(= -1\) &
\textbf{\(K_1 = -1\)} \\
Claim\(_2\) & ``I know I tend to underestimate myself'' & --- & --- \\
State\(_2\) & Claim\(_2\) correctly identifies \(K_1 = -1\) &
\(f_2(-1, \text{"may be wrong"})\) = meta-aligned & --- \\
\(K_2\) & \(g_2(\text{meta-aligned})\) & \(= +1\) &
\textbf{\(K_2 = +1\)} \\
\end{longtable}

\textbf{Interpretation}: Classic imposter syndrome with metacognitive
awareness---the subject knows they underestimate themselves. This
self-awareness (\(K_2 = +1\)) is a \textbf{teachable moment} for
intervention.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Summary of Examples:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1800}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2600}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\((K_0, K_1, K_2)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Key Insight
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Socratic Wisdom & \((0, +1, 0)\) & \#11 & Accurate ignorance
recognition \\
Deep Dunning-Kruger & \((-1, -1, -1)\) & \#27 & Triple misalignment,
intervention-resistant \\
Imposter Aware & \((+1, -1, +1)\) & \#7 & Self-aware underconfidence,
teachable \\
\end{longtable}

\textbf{Extension to Continuous Values:}

The 27-pattern table is a \textbf{discrete approximation}. For
continuous \(K_n \in [-1, 1]\), the taxonomy becomes a partition of the
unit cube \([-1, 1]^3\) into 27 regions, with boundaries at \(\pm 0.33\)
(see Threshold Justification below).

\textbf{Partial Order Among Patterns:}

Some patterns are ``better'' than others in terms of metacognitive
calibration:

\[\text{Calibration Quality} = \text{sign}(K_0 \cdot K_1) + \text{sign}(K_1 \cdot K_2)\]

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Quality
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Meaning
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example Patterns
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
+2 & Fully calibrated & \#1 (Perfect Calibration), \#11 (Socratic
Wisdom) \\
+1 & Partially calibrated & \#2, \#10 \\
0 & Mixed & \#5, \#14 \\
-1 & Partially miscalibrated & \#8, \#17 \\
-2 & Fully miscalibrated & \#18 (Deep Dunning-Kruger), \#27 (Entrenched
Error) \\
\end{longtable}

This ordering is \textbf{descriptive}, not normative; specific contexts
may value different patterns.

\textbf{Theoretical Value of \(K_2\):}

The third layer (\(K_2\)) enables modeling of \textbf{metacognitive
interventions} and their effectiveness:

\begin{itemize}
\tightlist
\item
  \(K_2 = +1\) with \(K_1 = -1\): Subject recognizes their metacognitive
  failure -\textgreater{} \textbf{teachable moment}
\item
  \(K_2 = -1\) with \(K_1 = -1\): Subject does not recognize their
  failure -\textgreater{} \textbf{resistant to intervention}
\end{itemize}

Higher-order reflection (\(K_2\), \(K_3\), \ldots) provides diagnostic
power for identifying when and how metacognitive correction is possible.

\hypertarget{continuous-to-categorical-mapping}{%
\subsubsection{Continuous-to-Categorical
Mapping}\label{continuous-to-categorical-mapping}}

The 27-pattern taxonomy uses prototypical anchors \(\{-1, 0, 1\}\). For
continuous \(K\) values, we define thresholds:

\begin{longtable}[]{@{}cc@{}}
\toprule\noalign{}
Continuous Range & Categorical Label \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K \in [-1, -0.33)\) & \(-1\) (Misconception/Misaligned) \\
\(K \in [-0.33, 0.33]\) & \(0\) (Ignorance/Uncertain) \\
\(K \in (0.33, 1]\) & \(1\) (Knowledge/Aligned) \\
\end{longtable}

\textbf{Rationale for \(\pm 0.33\) Default}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Symmetric Tercile}: Divides \([-1, 1]\) into three equal-width
  regions
\item
  \textbf{Neutral Zone}: The central region captures
  ``uncertain/indeterminate'' states
\item
  \textbf{Statistical Interpretation}: Under uniform prior, each
  category has equal probability
\item
  \textbf{Robustness}: Not sensitive to small estimation errors near
  boundaries
\end{enumerate}

\hypertarget{formal-justification-for-thresholds}{%
\paragraph{Formal Justification for
Thresholds}\label{formal-justification-for-thresholds}}

\textbf{Decision-Theoretic Grounding:}

The default thresholds \(\pm 0.33\) can be justified via decision
theory:

\textbf{Setup:} - Agent must classify \(K\) into \{misconception,
ignorance, knowledge\} - Utility function:
\(U(\text{action}, \text{true state})\) - Prior: Uniform over
\([-1, 1]\)

\textbf{Symmetric Loss:}

Under symmetric 0-1 loss (equal cost for all misclassifications):

\[\text{Optimal thresholds} = \arg\min_{\tau_1, \tau_2} E[\mathbb{1}(\text{misclassification})]\]

With uniform prior, this yields \(\tau_1 = -1/3 \approx -0.33\),
\(\tau_2 = 1/3 \approx 0.33\).

\textbf{Asymmetric Loss (Alternative):}

If false positives (claiming knowledge when ignorant) are more costly:

\[L(\text{classify as } 1 | \text{true } 0) = c > 1\]

Optimal thresholds shift: \(\tau_2 > 0.33\) (stricter knowledge
criterion).

\textbf{Proper Scoring Rule Connection:}

Under Brier score:

\[\text{Brier}(p, y) = (p - y)^2\]

The thresholds \(\pm 0.33\) correspond to the decision boundaries where
expected Brier score is minimized under uniform prior.

\textbf{Empirical Calibration (Future Work):}

For domain-specific applications: 1. Collect pilot data with known
ground truth 2. Compute ROC curve for each threshold 3. Select threshold
maximizing Youden's J or F1 score 4. Report sensitivity analysis across
threshold choices

\textbf{Alternative Thresholds}:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Approach & Thresholds & Use Case \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Tercile (default)} & \(\pm 0.33\) & Balanced classification \\
\textbf{Quartile} & \(\pm 0.5\) & Stricter knowledge/misconception
criteria \\
\textbf{ROC-optimized} & Data-driven & Maximize classification
accuracy \\
\textbf{Domain-specific} & Expert-defined & Match substantive theory \\
\end{longtable}

\textbf{Recommendation}: - Use \(\pm 0.33\) as default for comparability
across studies - Report sensitivity analysis with alternative thresholds
- For intervention design, consider ROC-optimized thresholds

\textbf{Reporting Recommendation}: - Report continuous \(K\) values for
statistical analysis - Use categorical labels for interpretation and
intervention design - Always include confidence intervals from
estimation

\textbf{Example}:

\[K_0 = 0.7, K_1 = -0.5, K_2 = 0.2\]

Categorical: \(K_0 = 1, K_1 = -1, K_2 = 0\) -\textgreater{} ``Knowing
Misconception, uncertain about meta''

This enables both fine-grained analysis and interpretable
classification.

\hypertarget{connection-with-metacognition-research}{%
\subsubsection{Connection with Metacognition
Research}\label{connection-with-metacognition-research}}

\textbf{Flavell (1979)} defined metacognition as ``the ability to
monitor and control one's own cognitive activities.'' The recursive
structure (\(K \to K(K) \to K(K(K))\)) formalizes this concept
mathematically.

\textbf{Nelson \& Narens (1990)} introduced the influential
\textbf{monitoring/control framework}, distinguishing between the
\textbf{object level} (cognitive processes) and the \textbf{meta level}
(monitoring and control of cognition). Our framework directly
corresponds to this structure:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Nelson \& Narens & This Framework \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Object level & State\(_0\) (first-order epistemic state) \\
Meta level (monitoring) & State\(_1\) (metacognitive state) \\
Control signal & Not modeled (orthogonal dimension) \\
\end{longtable}

Our \(K_0\) and \(K_1\) formalize the object/meta distinction with a
\textbf{single unified operator}, providing mathematical precision to
Nelson \& Narens' conceptual framework.

\textbf{Koriat (1993)} proposed the \textbf{cue-utilization theory},
explaining how confidence arises from accessibility and familiarity cues
rather than direct access to accuracy. This distinction between
cue-based confidence and actual accuracy corresponds precisely to our
separation of \(C\) (confidence) and \(K\) (epistemic state). Our
framework accommodates cue-based confidence as a component of \(C\),
while \(K\) measures the objective alignment between the subject's state
and reality.

\textbf{Kruger and Dunning (1999)} demonstrated that individuals with
low competence tend to overestimate their abilities. In our model, this
corresponds to \(K_0 = 0\) (ignorance) but \(K_1 = -1\) (misrecognition
of ignorance).

\textbf{Fleming and Daw (2017)} proposed a general Bayesian framework
for metacognitive computation, modeling metacognition as ``second-order
inference'' about the reliability of first-order cognitive processes.
Their distinction between first-order states and second-order inference
corresponds to our State\(_0\)/State\(_1\) hierarchy. While their
approach is Bayesian (modeling uncertainty about internal states) and
ours is observational (measuring alignment between claims and
performance), both frameworks capture the fundamental insight that
metacognition operates on a different level from cognition itself. The
K-C dissociation in our framework (epistemic state vs phenomenological
confidence) parallels their analysis of how confidence can diverge from
accuracy.

\textbf{Meta-d' (Maniscalco \& Lau, 2012)} provides a signal
detection-theoretic measure of metacognitive sensitivity. While meta-d'
quantifies \textbf{how well} subjects discriminate their own correct
from incorrect responses, our framework provides a \textbf{structural
vocabulary} for \textbf{what} metacognitive states exist. The two
approaches are complementary: meta-d' measures the quality of
monitoring; our \(K\) classifies the content of monitoring.

\textbf{Novel Contribution:} Our model provides a \textbf{structural
formalization} of recursive self-awareness that: 1. Unifies the
object/meta distinction (Nelson \& Narens) with a single recursive
operator 2. Separates epistemic state from cue-based confidence (Koriat)
3. Classifies all possible metacognitive configurations (27 patterns) 4.
Explicitly distinguishes ``Knowing Ignorance'' (Socratic wisdom) as a
high metacognitive achievement

\hypertarget{why-a-single-unified-k}{%
\subsubsection{Why a Single Unified K?}\label{why-a-single-unified-k}}

One might ask: ``Why not introduce separate operators for different
levels?'' The answer lies in the \textbf{universality of the epistemic
question}.

\textbf{Philosophical Motivation:}

The question ``Do I know?'' is the same question at every level: - ``Do
I know the answer?'' -\textgreater{} \(K_0\) - ``Do I know whether I
know the answer?'' -\textgreater{} \(K_1\) - ``Do I know whether I know
whether I know?'' -\textgreater{} \(K_2\)

\textbf{The question is universal. Only the object changes.}

A thermometer measures temperature. The same thermometer can measure the
temperature of water, air, or metal. We do not need separate
``water-thermometers'' and ``air-thermometers.'' The instrument is the
same; the objects differ.

Similarly, \(K\) is an \textbf{observation protocol}, not a mental
process. The same protocol applies to different objects (State\(_0\),
State\(_1\), State\(_2\)). Introducing separate operators (\(R\), \(E\),
etc.) would obscure this fundamental unity and sacrifice the elegance of
a single recursive structure.

\textbf{Practical Implementation:}

While the \textbf{semantic anchors} are shared (-1/0/1 for
misconception/ignorance/knowledge), the \textbf{measurement procedures}
\(K^{(n)}\) may differ:

\begin{longtable}[]{@{}cll@{}}
\toprule\noalign{}
Layer & Observable & Measurement Procedure \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K^{(0)}\) & Response vs Reference & Accuracy scoring \\
\(K^{(1)}\) & Claim vs State\(_0\) & Alignment scoring \\
\(K^{(2)}\) & Meta-claim vs State\(_1\) & Meta-alignment scoring \\
\end{longtable}

\textbf{Parameter Tying (Optional):}

For parsimony, one may assume: - Same noise model across layers - Same
link function (e.g., logistic)

Or allow layer-specific parameters if data supports it.

\textbf{The key constraint is shared anchor semantics, not identical
functional forms.}

\textbf{The beauty of \(K\) is its universality.} Knowing, not knowing,
and misunderstanding are universal human experiences. The same operator
captures them all.

\hypertarget{measurement-theory}{%
\subsection{Measurement Theory}\label{measurement-theory}}

This section describes how the theoretical constructs (\(K(x)\),
\(K(K(x))\)) can be operationalized and measured empirically.

\hypertarget{why-continuous-scale}{%
\subsubsection{Why Continuous Scale?}\label{why-continuous-scale}}

The continuous scale \([-1, 1]\) provides several advantages over binary
or categorical representations:

\textbf{1. Intermediate States:}

Captures partial knowledge, uncertain beliefs, and mixed states that
binary representations cannot express.

\begin{itemize}
\tightlist
\item
  Example: \(K_0 = 0.3\) represents ``mostly ignorant but with some
  relevant information''
\item
  Example: \(K_1 = -0.5\) represents ``moderate overconfidence, not
  extreme''
\end{itemize}

\textbf{2. Change Tracking:}

Enables measurement of \textbf{gradual transitions and intervention
effects}.

\begin{itemize}
\tightlist
\item
  Example: After metacognitive training, \(K_1\) moves from \(-0.8\) to
  \(-0.2\)

  \begin{itemize}
  \tightlist
  \item
    This shows improvement within the ``overconfidence'' category
  \item
    Binary classification would show no change (both are
    ``overconfident'')
  \end{itemize}
\end{itemize}

\textbf{3. Aggregation:}

Permits meaningful averaging across items, domains, or time points.

\begin{itemize}
\tightlist
\item
  Example: Average \(K_1\) across 50 items yields a stable estimate
\item
  Example: Compare \(K_1\) across domains (math vs.~history)
\end{itemize}

\textbf{4. Statistical Modeling:}

Compatible with standard regression, Bayesian inference, and
psychometric methods.

\begin{itemize}
\tightlist
\item
  Linear models: \(K_1 \sim K_0 + \text{training} + \epsilon\)
\item
  Hierarchical models: Subject-level and item-level random effects
\end{itemize}

\textbf{5. Geometric Extension (Future Work):}

Enables connection to \textbf{information geometry} and manifold-based
analysis:

\begin{itemize}
\tightlist
\item
  Cognitive states as points on a manifold
\item
  Interventions as trajectories
\item
  Distance metrics for comparing metacognitive profiles
\item
  Curvature analysis for stability of states
\end{itemize}

\textbf{Design Choice:}

The trichotomy \(\{-1, 0, 1\}\) represents \textbf{prototypical anchors}
on the continuous scale, not the only valid values. Researchers may: -
Use discrete elicitation and embed into continuous scale - Use
probabilistic elicitation for direct continuous measurement - Aggregate
discrete responses to obtain continuous estimates

\hypertarget{measurement-theoretic-interpretation}{%
\subsubsection{Measurement-Theoretic
Interpretation}\label{measurement-theoretic-interpretation}}

Mathematically, all epistemic states live on a \textbf{single continuous
scale}:

\[K_n \in [-1, 1] \quad (n = 0, 1, 2, \dots)\]

The values \(-1\), \(0\), and \(1\) function as \textbf{prototypical
anchor points} on this continuum:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3846}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3077}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3077}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Prototype
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) & Full correct knowledge & Subject's state is maximally aligned
with the chosen reference \\
\(0\) & Pure ignorance & Subject has no determinate stance regarding the
object \\
\(-1\) & Full misconception & Subject's state is maximally opposed to
the reference \\
\end{longtable}

All intermediate values in \((-1, 0)\) and \((0, 1)\) represent
\textbf{graded mixtures} of these prototypes (partial knowledge, partial
misconception, uncertainty, mixtures across items, etc.).

\textbf{Operationalization Options (always mapping back to
\([-1, 1]\)):}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Discrete elicitation -\textgreater{} Discrete embedding}: Use
  trichotomous responses (True/False/I don't know), then embed into
  \([-1, 1]\) via \(K(x) \in \{-1, 0, 1\}\) as prototype points.
\item
  \textbf{Probabilistic elicitation -\textgreater{} Continuous
  embedding}: Elicit a subjective probability \(p(x)\) and map it into
  \([-1, 1]\) using a proper scoring rule or a simple linear transform
  (e.g., centered Brier-type scores).
\item
  \textbf{Aggregation -\textgreater{} Continuous embedding}: Average
  prototype-valued \(K(x_i) \in \{-1, 0, 1\}\) across multiple items or
  contexts to obtain a continuous summary in \([-1, 1]\).
\end{enumerate}

Conceptually, the \textbf{continuum \([-1, 1]\) is primary}; the
trichotomy \(\{-1, 0, 1\}\) is a convenient way to name salient regions
on this line, not a separate codomain. Experimental designs may choose
discrete or continuous elicitation, but in all cases the resulting data
are interpreted as points (or distributions) on the same underlying
scale \([-1, 1]\).

\hypertarget{aggregation-rules-across-items}{%
\subsubsection{Aggregation Rules Across
Items}\label{aggregation-rules-across-items}}

For a subject responding to \(N\) items, we obtain item-level scores
\((K_0^{(i)}, K_1^{(i)}, K_2^{(i)})\) for \(i = 1, \ldots, N\).

\hypertarget{point-estimates}{%
\paragraph{Point Estimates}\label{point-estimates}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3056}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4444}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aggregate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Mean \(K_n\)} &
\(\bar{K}_n = \frac{1}{N} \sum_{i=1}^N K_n^{(i)}\) & Overall
epistemic/metacognitive level \\
\textbf{Weighted Mean} &
\(\bar{K}_n^w = \frac{\sum_i w_i K_n^{(i)}}{\sum_i w_i}\) &
Item-difficulty adjusted \\
\textbf{Distribution} & \(P(K_n = k)\) for \(k \in \{-1, 0, +1\}\) &
Pattern frequencies \\
\end{longtable}

\hypertarget{uncertainty-quantification}{%
\paragraph{Uncertainty
Quantification}\label{uncertainty-quantification}}

\textbf{Bootstrap Confidence Intervals:} 1. Resample \(N\) items with
replacement, \(B = 1000\) times 2. Compute \(\bar{K}_n^{(b)}\) for each
bootstrap sample 3. Report 95\% CI as
\([\bar{K}_n^{(0.025)}, \bar{K}_n^{(0.975)}]\)

\textbf{Reliability Threshold:} If CI width \(> 0.3\), interpret
aggregate \(K_n\) with caution.

\hypertarget{statistical-properties}{%
\paragraph{Statistical Properties}\label{statistical-properties}}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Property & Condition & Guarantee \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Consistency} & \(N \to \infty\) &
\(\bar{K}_n \to \mathbb{E}[K_n]\) \\
\textbf{Anchor Preservation} & All \(K_n^{(i)} = +1\) &
\(\bar{K}_n = +1\) \\
\textbf{Boundedness} & Always & \(\bar{K}_n \in [-1, +1]\) \\
\end{longtable}

\hypertarget{cross-item-coherence-check}{%
\paragraph{Cross-Item Coherence
Check}\label{cross-item-coherence-check}}

To verify that aggregation is meaningful:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Within-Subject Variance}: \(\text{Var}(K_n^{(i)})\) across
  items should be interpretable

  \begin{itemize}
  \tightlist
  \item
    High variance: Domain-specific metacognition
  \item
    Low variance: Trait-like metacognitive style
  \end{itemize}
\item
  \textbf{Correlation Structure}: \(\text{Cor}(K_0^{(i)}, K_1^{(i)})\)
  indicates coupling between knowledge and metacognition

  \begin{itemize}
  \tightlist
  \item
    Strong positive: Calibrated subject
  \item
    Near zero: Decoupled states (possible Dunning-Kruger)
  \end{itemize}
\end{enumerate}

\hypertarget{higher-layers-n-2-practical-considerations}{%
\subsubsection{Higher Layers (n \textgreater{} 2): Practical
Considerations}\label{higher-layers-n-2-practical-considerations}}

\hypertarget{diminishing-returns-hypothesis}{%
\paragraph{Diminishing Returns
Hypothesis}\label{diminishing-returns-hypothesis}}

As \(n\) increases, the marginal information provided by \(K_n\)
decreases:

\[\text{Var}(K_n | K_0, K_1, \ldots, K_{n-1}) \to 0 \text{ as } n \to \infty\]

\textbf{Rationale}: Higher-order metacognition becomes increasingly
abstract and harder to distinguish from lower layers.

\textbf{Note}: This is presented as a \textbf{hypothesis} based on
theoretical considerations. Empirical validation is deferred to future
simulation studies. No direct verification data currently exists for
this claim.

\hypertarget{elicitation-challenges-for-claim_n-n-2}{%
\paragraph{\texorpdfstring{Elicitation Challenges for Claim\(_n\) (n
\textgreater{}
2)}{Elicitation Challenges for Claim\_n (n \textgreater{} 2)}}\label{elicitation-challenges-for-claim_n-n-2}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1842}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.6316}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Claim
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Elicitation Difficulty
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(n = 1\) & ``Do I know?'' & Low (familiar question) \\
\(n = 2\) & ``Is my self-assessment accurate?'' & Medium (requires
reflection) \\
\(n = 3\) & ``Is my assessment of my self-assessment accurate?'' & High
(conceptually recursive) \\
\(n > 3\) & \ldots{} & Very high (risk of tautological responses) \\
\end{longtable}

\hypertarget{demand-characteristics}{%
\paragraph{Demand Characteristics}\label{demand-characteristics}}

Higher-order claims risk: 1. \textbf{Tautological responses}: ``If I
thought my self-assessment was wrong, I would have changed it'' 2.
\textbf{Ceiling effects}: Most subjects claim their assessments are
accurate 3. \textbf{Cognitive overload}: Difficulty distinguishing
layers

\hypertarget{recommended-scope}{%
\paragraph{Recommended Scope}\label{recommended-scope}}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Recommended Max Layer \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Standard assessment & \(K_0\), \(K_1\) \\
Metacognitive research & \(K_0\), \(K_1\), \(K_2\) \\
Specialized studies & Up to \(K_3\) with careful protocol design \\
\end{longtable}

\textbf{Practical Guidance}: For most applications, \(K_0\) and \(K_1\)
provide sufficient diagnostic information. \(K_2\) adds value for
distinguishing ``teachable'' from ``resistant'' misconceptions. Beyond
\(K_2\), empirical justification should precede deployment.

\hypertarget{person-level-aggregation-of-k_n}{%
\subsubsection{\texorpdfstring{Person-Level Aggregation of
\(K_n\)}{Person-Level Aggregation of K\_n}}\label{person-level-aggregation-of-k_n}}

\hypertarget{item-level-to-person-level}{%
\paragraph{Item-Level to
Person-Level}\label{item-level-to-person-level}}

Given \(m\) items with scores
\(K_n^{(1)}, K_n^{(2)}, \ldots, K_n^{(m)}\), define person-level index:

\[\bar{K}_n = \frac{1}{m} \sum_{i=1}^{m} K_n^{(i)}\]

\hypertarget{psychometric-properties}{%
\paragraph{Psychometric Properties}\label{psychometric-properties}}

\textbf{Reliability}:

Using Cronbach's alpha analog:

\[\alpha_{K_n} = \frac{m}{m-1} \left(1 - \frac{\sum_i \text{Var}(K_n^{(i)})}{\text{Var}(\sum_i K_n^{(i)})}\right)\]

Target: \(\alpha_{K_n} > 0.7\) for adequate reliability.

\textbf{Alternative: Split-Half Reliability}:

\[r_{K_n} = \text{Cor}(\bar{K}_n^{\text{odd}}, \bar{K}_n^{\text{even}})\]

With Spearman-Brown correction for full-test reliability.

\hypertarget{measurement-invariance}{%
\paragraph{Measurement Invariance}\label{measurement-invariance}}

For cross-group comparisons (e.g., experts vs novices), test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Configural invariance}: Same factor structure across groups
\item
  \textbf{Metric invariance}: Same \(f_n\) loadings across groups
\item
  \textbf{Scalar invariance}: Same intercepts (anchor alignment) across
  groups
\end{enumerate}

\textbf{Implementation}: Use multi-group confirmatory factor analysis
(CFA) with \(K_n\) as latent variable.

\hypertarget{irt-based-aggregation}{%
\paragraph{IRT-Based Aggregation}\label{irt-based-aggregation}}

For more sophisticated aggregation:

\[\bar{K}_n = \tanh(\hat{\theta}_n)\]

Where \(\hat{\theta}_n\) is the latent trait estimated via IRT model on
\(K_n^{(i)}\) items.

\textbf{Advantages}: - Accounts for item difficulty/discrimination -
Provides standard errors for \(\bar{K}_n\) - Enables adaptive testing
designs

\hypertarget{unified-estimation-pipeline}{%
\subsubsection{Unified Estimation
Pipeline}\label{unified-estimation-pipeline}}

\hypertarget{overview}{%
\paragraph{Overview}\label{overview}}

The complete pipeline integrates categorical and continuous
interpretations:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4118}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4510}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Stage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Option A (Discrete)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Option B (Continuous)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Input} & (Response, Claim\(_1\), Claim\(_2\), \ldots, Reference)
& Same \\
\textbf{\(f_n\) output} & \(\{-1, 0, +1\}\) & \([-1, +1]\) \\
\textbf{\(g_n\)} & identity & link function (e.g., tanh) \\
\textbf{\(\hat{K}\)} & identity & normalizer (optional) \\
\textbf{Output} & \(K_0, K_1, K_2, \ldots \in \{-1, 0, +1\}\) &
\(K_0, K_1, K_2, \ldots \in [-1, +1]\) \\
\end{longtable}

\hypertarget{option-a-discrete-pipeline}{%
\paragraph{Option A: Discrete
Pipeline}\label{option-a-discrete-pipeline}}

\textbf{Step 1}: Compute \(K_0^{\text{cat}} \in \{-1, 0, +1\}\) from
(Response, Reference)

\textbf{Step 2}: Compute \(K_1^{\text{cat}} \in \{-1, 0, +1\}\) from
(Claim\(_1\), \(K_0^{\text{cat}}\)) via \(f_1\) table

\textbf{Step 3}: Compute \(K_2^{\text{cat}} \in \{-1, 0, +1\}\) from
(Claim\(_2\), \(K_1^{\text{cat}}\)) via \(f_2\) table

\textbf{Output}: Categorical pattern
\((K_0^{\text{cat}}, K_1^{\text{cat}}, K_2^{\text{cat}}) \in \{-1, 0, +1\}^3\)

\hypertarget{option-b-continuous-pipeline}{%
\paragraph{Option B: Continuous
Pipeline}\label{option-b-continuous-pipeline}}

\textbf{Step 1}: Estimate latent \(\theta_0\) via IRT; compute
\(K_0 = \tanh(\theta_0)\)

\textbf{Step 2}: Estimate meta-d' from (Response, Claim\(_1\)) via SDT;
compute \(K_1 = \tanh(\text{meta-d}'/2)\)

\textbf{Step 3}: Estimate \(K_2\) via test-retest or Claim\(_2\)
alignment

\textbf{Output}: Continuous scores \((K_0, K_1, K_2) \in [-1, +1]^3\)

\hypertarget{hybrid-pipeline}{%
\paragraph{Hybrid Pipeline}\label{hybrid-pipeline}}

For practical use, combine:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Categorical for pattern classification}: Which of the 27
  patterns?
\item
  \textbf{Continuous for severity/reliability}: How far from anchors?
  How stable?
\end{enumerate}

\[K_n^{\text{hybrid}} = (K_n^{\text{cat}}, K_n^{\text{cont}}, \text{SE}(K_n^{\text{cont}}))\]

Where SE is the standard error from the continuous estimation.

\textbf{Use Case Recommendations}:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Application & Recommended Pipeline \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Quick screening & Option A (Discrete) \\
Research analysis & Option B (Continuous) \\
Clinical/educational & Hybrid \\
LLM evaluation & Option A with Continuous extension \\
\end{longtable}

\hypertarget{estimation-methods-for-k-values}{%
\subsubsection{Estimation Methods for K
Values}\label{estimation-methods-for-k-values}}

The \(K\) framework specifies \textbf{what to measure} (epistemic state
coordinates); for \textbf{how to estimate}, we adopt established
psychometric and signal-detection methods as ``plug-in'' engines. This
separation preserves our conceptual contribution while leveraging
validated estimation machinery.

\hypertarget{k_0-estimation-first-order-epistemic-state}{%
\paragraph{\texorpdfstring{\(K_0\) Estimation (First-Order Epistemic
State)}{K\_0 Estimation (First-Order Epistemic State)}}\label{k_0-estimation-first-order-epistemic-state}}

\textbf{Method}: Item Response Theory (2-Parameter Logistic Model)

\[P(\text{correct} | \theta_s, a_i, b_i) = \frac{1}{1 + e^{-a_i(\theta_s - b_i)}}\]

\textbf{Mapping to \(K_0\)}:

\[K_0 = 2 \cdot \Phi(\theta_s) - 1\]

Where \(\Phi\) is the standard normal CDF, ensuring \(K_0 \in [-1, 1]\).

\hypertarget{formal-derivation-k_0-approx-tanhtheta}{%
\subparagraph{\texorpdfstring{Formal Derivation:
\(K_0 \approx \tanh(\theta)\)}{Formal Derivation: K\_0 \textbackslash approx \textbackslash tanh(\textbackslash theta)}}\label{formal-derivation-k_0-approx-tanhtheta}}

The Executive Summary states \(K_0 \approx \tanh(\theta)\). We now
provide the formal derivation from the 2PL IRT model.

\textbf{Step 1: Convert probability to \([-1, 1]\) scale}

Given the 2PL response probability:
\[P = \frac{1}{1 + e^{-a(\theta - b)}}\]

We transform to a signed scale:
\[K_0^* = 2P - 1 = \frac{2}{1 + e^{-a(\theta - b)}} - 1 = \frac{1 - e^{-a(\theta - b)}}{1 + e^{-a(\theta - b)}}\]

\textbf{Step 2: Recognize hyperbolic tangent identity}

The hyperbolic tangent satisfies:
\[\tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1} = \frac{1 - e^{-2x}}{1 + e^{-2x}}\]

\textbf{Step 3: Match exponents}

Comparing forms, we identify:
\[K_0^* = \tanh\left(\frac{a(\theta - b)}{2}\right)\]

\textbf{Full Form}: \[K_0 = \tanh\left(\frac{a(\theta - b)}{2}\right)\]

\textbf{Simplified Form (under standardization assumptions)}:

When \(a = 2\) (unit discrimination) and \(b = 0\) (centered
difficulty): \[K_0 \approx \tanh(\theta)\]

\textbf{Dependency Note}: This mapping is item-parameter dependent: -
High \(a\) (discriminating items) → sharper transition near
\(\theta = b\) - High \(b\) (difficult items) → shift toward lower
\(K_0\) for fixed \(\theta\)

For aggregate \(K_0\) across items with varying \((a_i, b_i)\):
\[\bar{K}_0 = \frac{1}{N} \sum_{i=1}^{N} \tanh\left(\frac{a_i(\theta - b_i)}{2}\right)\]

Or estimate \(\theta\) via standard IRT procedures and apply the mapping
post-hoc.

\textbf{Misconception Detection}: - High confidence + incorrect
-\textgreater{} \(K_0 = -1\) - Operationalized via Confidence-Accuracy
calibration error

\hypertarget{k_1-estimation-metacognitive-alignment}{%
\paragraph{\texorpdfstring{\(K_1\) Estimation (Metacognitive
Alignment)}{K\_1 Estimation (Metacognitive Alignment)}}\label{k_1-estimation-metacognitive-alignment}}

\textbf{Method A}: Phi Coefficient

\[\phi = \frac{n_{11} n_{00} - n_{10} n_{01}}{\sqrt{(n_{11}+n_{10})(n_{01}+n_{00})(n_{11}+n_{01})(n_{10}+n_{00})}}\]

\textbf{Handling 3-Value State\(_0\)}:

State\(_0\) has three outcomes \{correct, incorrect, absent\}. For Phi
(\(2 \times 2\)), we binarize:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Binarization Strategy
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive (\(K_0 > 0\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Negative (\(K_0 \leq 0\))
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Strategy A (Strict)} & correct only & incorrect + absent \\
\textbf{Strategy B (Lenient)} & correct + absent & incorrect only \\
\textbf{Strategy C (Exclude)} & correct & incorrect (exclude absent) \\
\end{longtable}

\textbf{Recommended}: Strategy A (Strict) --- aligns with the
interpretation that ``I know'' should predict correctness, not just
absence of misconception.

\textbf{Cell Definitions (Strategy A)}: - \(n_{11}\): correct + ``I
know'' - \(n_{00}\): (incorrect OR absent) + ``I don't know'' -
\(n_{10}\): correct + ``I don't know'' - \(n_{01}\): (incorrect OR
absent) + ``I know''

\textbf{Mapping}: \(K_1 = \phi\) (already in \([-1, 1]\))

\textbf{Interpretation}: - \(\phi = 1\): Perfect metacognitive alignment
- \(\phi = 0\): No relationship (random) - \(\phi = -1\): Perfect
anti-alignment (systematic miscalibration)

\textbf{Method B}: meta-d' Ratio (Signal Detection Theory)

\[K_1 = f\left(\frac{\text{meta-d}'}{d'}\right)\]

Where: - meta-d' = metacognitive sensitivity (Maniscalco \& Lau, 2012) -
\(d'\) = first-order sensitivity - \(f\): bounding function to ensure
output in \([-1, 1]\)

\textbf{Bounding Function Options}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Properties
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{tanh} (default) & \(\tanh(r)\) & Smooth, symmetric, saturates at
\(\pm 1\) \\
\textbf{Scaled CDF} & \(2\Phi(r) - 1\) & Probabilistic interpretation \\
\textbf{Clipped linear} & \(\max(-1, \min(1, r))\) & Simple, preserves
scale near 0 \\
\end{longtable}

\textbf{Rationale for tanh (default)}: - Smooth monotonic transformation
- Natural saturation at extreme values - Widely used in neural network
literature - \textbf{Alternative-agnostic}: Results qualitatively
similar across choices

\textbf{Sensitivity Analysis Recommendation}: Report results with at
least two bounding functions to confirm robustness.

\textbf{Choice Guidance}: - Use Phi for simplicity and interpretability
(no bounding needed) - Use meta-d'/d' for SDT-compatible analyses (with
explicit bounding function)

\hypertarget{k_2-estimation-higher-order-alignment}{%
\paragraph{\texorpdfstring{\(K_2\) Estimation (Higher-Order
Alignment)}{K\_2 Estimation (Higher-Order Alignment)}}\label{k_2-estimation-higher-order-alignment}}

\(K_2\) measures meta-metacognitive alignment: does the subject
accurately assess their own metacognitive accuracy (\(K_1\))? We present
three candidate methods with a recommended default.

\hypertarget{method-a-test-retest-stability}{%
\subparagraph{Method A: Test-Retest
Stability}\label{method-a-test-retest-stability}}

\[K_2^{(\text{stability})} = \text{Cor}(K_1^{(t_1)}, K_1^{(t_2)})\]

Where \(K_1\) is measured at two time points. High stability
(\(K_2 \to +1\)) indicates consistent metacognitive self-assessment.

\textbf{Interpretation}: This operationalizes \(K_2\) as
\textbf{reliability of \(K_1\)} rather than accuracy.

\textbf{Requirements}: Repeated measurement at \(t_1\), \(t_2\)
(recommended: 2-4 weeks apart)

\hypertarget{method-b-higher-order-claim-alignment-primary}{%
\subparagraph{Method B: Higher-Order Claim Alignment
(PRIMARY)}\label{method-b-higher-order-claim-alignment-primary}}

\[K_2^{(\text{claim})} = \mathbb{1}[\text{Claim}_2 \text{ matches actual } K_1] \cdot 2 - 1\]

Where Claim\(_2\) is the subject's belief about their own metacognitive
accuracy.

\textbf{Implementation}: 1. Compute \(K_1\) from (Response, Claim\(_1\),
Reference) 2. Elicit Claim\(_2\): ``Is your self-assessment accurate?''
3. Compare Claim\(_2\) to actual \(K_1\)

\textbf{Aggregate Form} (across items):
\[K_2 = 2 \cdot P(\text{Claim}_2 \text{ matches } \text{State}_1) - 1\]

\hypertarget{method-c-hierarchical-bayesian-reliability}{%
\subparagraph{Method C: Hierarchical Bayesian
Reliability}\label{method-c-hierarchical-bayesian-reliability}}

Model \(K_1^{(i)}\) as noisy observations of a latent true \(K_1^*\):

\[K_1^{(i)} | K_1^* \sim \mathcal{N}(K_1^*, \sigma^2)\]

Then:
\[K_2 = 1 - \frac{\text{Var}(K_1^{(i)} | K_1^*)}{\text{Var}(K_1^{(i)})} = \frac{\text{Var}(K_1^*)}{\text{Var}(K_1^{(i)})}\]

This is the \textbf{reliability coefficient} (analogous to Cronbach's
\(\alpha\)).

\textbf{Implementation}: Hierarchical Bayesian GLM (cf.~HiBayES, Fleming
\& Daw, 2017)

\[\text{Claim}_2 | \text{State}_1 \sim \text{Bernoulli}(\sigma(\alpha_s + \beta_i + \gamma_{s,i}))\]

\hypertarget{recommended-default-and-validation-strategy}{%
\subparagraph{Recommended Default and Validation
Strategy}\label{recommended-default-and-validation-strategy}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2941}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1765}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Measures
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Requires
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{B (Claim)} & Accuracy of meta-metacognition & Explicit
Claim\(_2\) & \textbf{Primary} \\
A (Stability) & Reliability of \(K_1\) & Repeated measurement &
\textbf{Validation} \\
C (Bayesian) & Signal-to-noise ratio & Multiple items (\(N \geq 20\)) &
\textbf{Robustness check} \\
\end{longtable}

\textbf{Rationale for Method B as Primary}: - Direct operationalization
of the theoretical construct (meta-metacognitive accuracy) -
Single-session administration (no need for retest) - Consistent with the
MAT protocol structure (claim-based measurement)

\textbf{Validation Protocol}: 1. Compute \(K_2\) via Method B (primary
estimate) 2. If retest data available, validate with Method A (expected
correlation \(r > 0.5\)) 3. For large \(N\), verify with Method C
(expected convergence)

\hypertarget{summary-table-estimation-methods}{%
\paragraph{Summary Table: Estimation
Methods}\label{summary-table-estimation-methods}}

\begin{longtable}[]{@{}cllc@{}}
\toprule\noalign{}
Layer & Observable & Method & Output \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & Response vs Reference & IRT (2PL) & \([-1, 1]\) \\
\(K_1\) & Claim\(_1\) vs State\(_0\) & Phi / meta-d'/d' & \([-1, 1]\) \\
\(K_2\) & Claim\(_2\) vs State\(_1\) & Hierarchical Bayes &
\([-1, 1]\) \\
\(C\) & Self-reported confidence & Direct elicitation & \([0, 1]\) \\
\end{longtable}

\hypertarget{identifiability-analysis}{%
\paragraph{Identifiability Analysis}\label{identifiability-analysis}}

\textbf{Definition}: A parameter \(\theta\) is \textbf{identifiable} if
there exists a measurable function \(\hat{\theta}\) such that, as
\(N \to \infty\), \(\hat{\theta} \xrightarrow{p} \theta\) (i.e., the
estimator converges in probability to the true value).

\textbf{Identifiability Conditions by Layer}:

\textbf{\(K_0\) Identifiability}: - \textbf{Requires}: Multiple items
per subject to separate subject ability from item difficulty -
\textbf{Condition}: Variance in item difficulties
\(\text{Var}(b_i) > 0\) - \textbf{Minimum}: \(N \geq 10\) items for
stable 2PL estimation

\textbf{\(K_1\) Identifiability}: - \textbf{Requires}: Multiple trials
per subject with varying \(K_0\) outcomes - \textbf{Condition}: Both
correct and incorrect responses must occur - \textbf{Minimum}:
\(N \geq 15\) trials with both positive and negative \(K_0\) outcomes

\textbf{\(K_2\) Identifiability}: - \textbf{Requires}: Observed \(K_1\)
variation across trials - \textbf{Condition}: Non-degenerate \(K_1\)
distribution (not all perfect or all random) - \textbf{Minimum}:
\(N \geq 20\) trials for hierarchical Bayesian estimation

\textbf{Non-Identifiability Cases}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scenario
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Observable Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diagnosis
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Ceiling \(K_0\) & All items correct & Cannot estimate \(K_1\) (no error
signal) \\
Floor \(K_0\) & All items incorrect & Cannot distinguish misconception
from guessing \\
Perfect \(K_1\) & All metacognitive claims correct & \(K_2\) undefined
(no calibration error) \\
Random \(K_1\) & \(\phi = 0\) & Insufficient metacognitive signal for
\(K_2\) \\
\end{longtable}

\textbf{Practical Guideline}:

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1739}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1739}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Minimum N
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Recommended N
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Estimation Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Identifiability Check
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & 10 & 30+ & IRT 2PL & SE(\(\theta_s\)) \textless{} 0.5 \\
\(K_1\) & 15 & 25+ & Phi / meta-d' & \(n_{ij} \geq 5\) per cell \\
\(K_2\) & 20 & 40+ & Hierarchical Bayes & Rhat \textless{} 1.1, ESS
\textgreater{} 100 \\
\end{longtable}

\textbf{Note}: These are minimum requirements. For individual-level
claims about specific subjects, larger sample sizes or domain-specific
validations are recommended.

\hypertarget{identifiability-under-latent-variable-model}{%
\paragraph{Identifiability under Latent Variable
Model}\label{identifiability-under-latent-variable-model}}

Given the latent variable formulation
\(K_n = \tanh(\beta_n \cdot \theta_n)\), we address identifiability:

\textbf{Location-Scale Indeterminacy:}

\(\theta_n\) is identified only up to a scale factor (since
\(\tanh(\beta \theta) = \tanh((\beta c)(\theta/c))\) for any \(c > 0\)).

\textbf{Resolution:} Fix \(\beta = 1\) (standard parameterization) or
anchor to a reference population.

\textbf{Finite-Sample Identifiability:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3585}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5094}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Data Requirements
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Identifiability Condition
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & \(\geq 10\) items with known ground truth & Variance in ground
truth states \\
\(K_1\) & \(K_0\) estimates + self-assessments & Variance in
\((K_0, \text{Claim}_1)\) pairs \\
\(K_2\) & \(K_1\) estimates + meta-self-assessments & Non-degenerate
\((K_1, \text{Claim}_2)\) \\
\end{longtable}

\textbf{Practical Guideline:} For reliable estimation, collect data
across the full range of \(K_n\) values. Pure cases (all \(+1\) or all
\(-1\)) provide no information about the link function shape.

\hypertarget{reliability-and-validity-guidelines}{%
\subsubsection{Reliability and Validity
Guidelines}\label{reliability-and-validity-guidelines}}

\hypertarget{test-retest-reliability}{%
\paragraph{Test-Retest Reliability}\label{test-retest-reliability}}

\textbf{Concern:} Are \(K_n\) scores stable over time (assuming no true
change)?

\textbf{Protocol:} 1. Administer MAT at time \(t_1\) 2. Re-administer at
\(t_2\) (recommended: 2-4 weeks) 3. Compute intraclass correlation (ICC)
for \(K_0\), \(K_1\), \(K_2\)

\textbf{Expected Results:}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Layer & Expected ICC & Rationale \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & 0.7-0.9 & Knowledge is relatively stable \\
\(K_1\) & 0.5-0.8 & Metacognition may fluctuate with context \\
\(K_2\) & 0.4-0.7 & Meta-metacognition is more variable \\
\end{longtable}

\textbf{Interpretation:} ICC \textgreater{} 0.7 indicates acceptable
stability; lower values suggest either measurement noise or genuine
state instability.

\hypertarget{inter-rater-reliability}{%
\paragraph{Inter-Rater Reliability}\label{inter-rater-reliability}}

\textbf{Concern:} Do different observers compute the same \(K_n\) from
identical data?

\textbf{Protocol:} 1. Two+ independent raters apply the \(f_n, g_n\)
mappings 2. Compute Cohen's \(\kappa\) for discretized \(K_n\) 3.
Compute ICC for continuous \(K_n\) estimates

\textbf{Expected Results:}

Given the deterministic nature of \(f_n\) and \(g_n\), inter-rater
agreement should be \textbf{near-perfect} (\(\kappa > 0.9\)) for
unambiguous responses. Disagreements indicate: - Ambiguous claim
interpretation (refine claim vocabulary) - Reference disagreement
(clarify ground truth designation)

\hypertarget{split-half-reliability}{%
\paragraph{Split-Half Reliability}\label{split-half-reliability}}

\textbf{Concern:} Is \(K_n\) estimation internally consistent across
item subsets?

\textbf{Protocol:} 1. Randomly split items into two halves (A and B) 2.
Compute \(K_n^{(A)}\) and \(K_n^{(B)}\) separately 3. Correlate the two
estimates; apply Spearman-Brown correction

\textbf{Expected Results:}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Layer & Expected Split-Half \(r\) & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_0\) & 0.7-0.9 & High internal consistency \\
\(K_1\) & 0.5-0.8 & Moderate; depends on item heterogeneity \\
\(K_2\) & 0.4-0.7 & Lower; meta-meta states are more variable \\
\end{longtable}

\textbf{Guideline:} Spearman-Brown corrected \(r > 0.7\) indicates
acceptable internal consistency.

\hypertarget{measurement-invariance-1}{%
\paragraph{Measurement Invariance}\label{measurement-invariance-1}}

\textbf{Concern:} Do \(K_n\) scores have the same meaning across
different populations or item sets?

\textbf{Protocol:} 1. Administer MAT to multiple groups (e.g., experts
vs novices, domains A vs B) 2. Fit latent variable model
\(K_n = h(\theta_n)\) separately per group 3. Test whether link function
parameters (\(\beta\)) are equivalent

\textbf{Interpretation:} - Equivalent \(\beta\) across groups
-\textgreater{} Scores are comparable - Different \(\beta\)
-\textgreater{} Group-specific calibration needed; interpret
within-group only

\hypertarget{cross-study-comparability}{%
\paragraph{Cross-Study Comparability}\label{cross-study-comparability}}

\textbf{Concern:} Can \(K_n\) scores from different studies be
meaningfully compared?

\textbf{Requirements for Comparability:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Requirement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Verification
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Same \(f_n\) specification} & Identical claim vocabulary and
alignment rules & Document and share protocol \\
\textbf{Comparable reference standards} & Similar ground-truth
designation criteria & Report reference source \\
\textbf{Equivalent \(\hat{K}\) parameterization} & Same link function
and \(\beta\) & Fix \(\beta = 1\) or anchor to common scale \\
\end{longtable}

\textbf{Recommended Practice:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Protocol registration}: Pre-specify \(f_n\), \(g_n\), and
  \(\hat{K}\) before data collection
\item
  \textbf{Anchor items}: Include common items across studies for
  calibration
\item
  \textbf{Report uncertainty}: Provide confidence intervals for \(K_n\)
  estimates
\end{enumerate}

\textbf{When Comparability Fails:}

If studies use different references or \(f_n\) specifications, direct
\(K_n\) comparison is invalid. Instead: - Report within-study patterns
(e.g., proportion of Dunning-Kruger patterns) - Compare relative
rankings, not absolute \(K_n\) values

\hypertarget{continuous-estimation-of-kkx}{%
\subsubsection{\texorpdfstring{Continuous Estimation of
\(K(K(x))\)}{Continuous Estimation of K(K(x))}}\label{continuous-estimation-of-kkx}}

The categorical inference of \(K(K(x))\) from a single ``Do you know?''
claim is a \textbf{simplified operationalization}. For more robust
measurement, we propose:

\textbf{Option 1: Aggregation Across Items}

For a subject responding to multiple items within a domain:

\[K(K)_{aggregate} = 2 \cdot P(\text{meta-claim matches actual state}) - 1\]

where \(P\) is estimated across all items. This yields a continuous
value in \([-1, 1]\).

\textbf{Option 2: Hierarchical Bayesian Estimation}

Model \(K(K(x))\) as a latent variable with: - Prior distribution over
subjects - Item-level random effects - Observation model linking latent
\(K(K(x))\) to categorical claims

This approach accommodates noise, individual differences, and item
difficulty.

\textbf{Option 3: Probabilistic Elicitation}

Instead of categorical ``Yes/No/Unsure'', elicit: - ``How confident are
you that your previous answer was correct?'' (0-100\%)

Map this to \(K(K(x))\) via a proper scoring rule or calibration
analysis.

\hypertarget{operational-semantics-for-intermediate-values}{%
\subsubsection{Operational Semantics for Intermediate
Values}\label{operational-semantics-for-intermediate-values}}

\textbf{The Question:} What does \(K_n = 0.6\) mean? The anchors
(\(-1\), \(0\), \(+1\)) have clear semantics, but intermediate values
require interpretation.

\textbf{Latent Variable Model:}

We interpret \(K_n\) as the observable output of a latent alignment
variable \(\theta_n \in \mathbb{R}\), mapped to \([-1, 1]\) via a
monotonic link function:

\[K_n = h(\theta_n) = \tanh(\beta \cdot \theta_n)\]

where: - \(\theta_n\): Latent alignment strength (unbounded) -
\(\beta > 0\): Sensitivity parameter (determines curve steepness) -
\(h\): Link function satisfying \(h(0) = 0\),
\(\lim_{\theta \to \pm\infty} h(\theta) = \pm 1\)

\textbf{Interpretation of Intermediate Values:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3929}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3750}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(K_n\) Value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Latent Interpretation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Operational Meaning
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_n = +1\) & \(\theta_n \to +\infty\) & Perfect alignment/knowledge \\
\(K_n = +0.6\) & \(\theta_n > 0\) (moderate) & Probable alignment with
uncertainty \\
\(K_n = 0\) & \(\theta_n = 0\) & No systematic alignment or
misalignment \\
\(K_n = -0.6\) & \(\theta_n < 0\) (moderate) & Probable misalignment
with uncertainty \\
\(K_n = -1\) & \(\theta_n \to -\infty\) & Perfect
misalignment/misconception \\
\end{longtable}

\textbf{Alternative Link Functions:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3226}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2903}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3871}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Function
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Properties
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{tanh} (default) & \(\tanh(\beta \theta)\) & Smooth, symmetric,
unbounded input \\
\textbf{Scaled probit} & \(2\Phi(\theta) - 1\) & Probabilistic
interpretation \\
\textbf{Clipped linear} & \(\max(-1, \min(1, \theta))\) & Simple,
piecewise linear \\
\end{longtable}

The choice of link function is an \textbf{empirical question} to be
settled by future validation studies.

\textbf{Why This Matters:}

Without latent variable semantics, intermediate values risk being
``nominal with three anchors.'' The link function approach provides: 1.
\textbf{Continuous gradation}: Values between anchors have principled
meaning 2. \textbf{Estimation targets}: \(\theta_n\) can be estimated
via maximum likelihood 3. \textbf{Uncertainty quantification}: Standard
errors on \(\hat{\theta}_n\) yield confidence intervals for \(K_n\)

\hypertarget{continuous-k_n-values-via-proper-scoring-rules}{%
\subsubsection{\texorpdfstring{Continuous \(K_n\) Values via Proper
Scoring
Rules}{Continuous K\_n Values via Proper Scoring Rules}}\label{continuous-k_n-values-via-proper-scoring-rules}}

For discrete elicitation mapped to continuous \(K_n\), we define
principled estimators based on strictly proper scoring rules. This
ensures that intermediate values (e.g., \(K_0 = 0.6\)) are not arbitrary
but derive from established accuracy metrics.

\hypertarget{k_0-continuous-estimation}{%
\paragraph{\texorpdfstring{\(K_0\) Continuous
Estimation}{K\_0 Continuous Estimation}}\label{k_0-continuous-estimation}}

\textbf{Option A: Brier-Based Embedding}

Given a response \(r\) and reference \(t\), with confidence
\(c \in [0, 1]\):

\[K_0 = \begin{cases}
2c - 1 & \text{if } r = t \text{ (correct)} \\
0 & \text{if } r = \text{abstain} \\
-(2c - 1) & \text{if } r \neq t \text{ (incorrect)}
\end{cases}\]

This maps high-confidence correct to \(K_0 \to +1\), high-confidence
incorrect to \(K_0 \to -1\), and low-confidence or abstention to
\(K_0 \to 0\).

\textbf{Option B: Proper Score Centering}

Using Brier score \(B = (c - \mathbb{1}[\text{correct}])^2\):

\[K_0 = 1 - 2B\]

This yields \(K_0 = +1\) for perfect calibration on correct,
\(K_0 = -1\) for perfect miscalibration.

\textbf{Derivation}: The Brier score \(B \in [0, 1]\) is a strictly
proper scoring rule. The transformation \(K_0 = 1 - 2B\) linearly
rescales to \([-1, 1]\), preserving properness.

\hypertarget{k_1-continuous-estimation}{%
\paragraph{\texorpdfstring{\(K_1\) Continuous
Estimation}{K\_1 Continuous Estimation}}\label{k_1-continuous-estimation}}

\textbf{Meta-d' Based}:

\[K_1 = \tanh\left(\frac{\text{meta-d}'}{2}\right)\]

Where meta-d' is the signal-detection sensitivity for Type-2 decisions
(discriminating correct from incorrect responses).

\textbf{Alignment Score Based}:

For item \(i\) with actual \(K_0^{(i)}\) and claimed state
\(\tilde{K}_0^{(i)}\):

\[K_1 = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[K_0^{(i)} \cdot \tilde{K}_0^{(i)} > 0] \cdot 2 - 1\]

This yields \(K_1 = +1\) for perfect alignment, \(K_1 = -1\) for
systematic misalignment.

\hypertarget{properties-of-continuous-estimators}{%
\paragraph{Properties of Continuous
Estimators}\label{properties-of-continuous-estimators}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2381}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4048}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Property
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_0\) (Brier)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_1\) (meta-d')
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Anchor Preservation} & \(K_0 \in \{-1, 0, +1\}\) at extremes &
\(K_1 \in \{-1, 0, +1\}\) at extremes \\
\textbf{Monotonicity} & Increases with accuracy × confidence & Increases
with metacognitive sensitivity \\
\textbf{Properness} & Derived from strictly proper Brier & meta-d' is
bias-free under SDT assumptions \\
\textbf{Continuity} & Continuous in \(c\) & Continuous in meta-d' \\
\end{longtable}

\hypertarget{the-challenge-of-measuring-second-order-states}{%
\subsubsection{The Challenge of Measuring Second-Order
States}\label{the-challenge-of-measuring-second-order-states}}

\(K(K(x))\) is a \textbf{second-order epistemic state}: it represents
the subject's recognition of their own first-order state \(K(x)\). We
cannot directly observe \(K(K(x))\); we must infer it from observable
behavior.

\hypertarget{operational-definition-of-confidence-c}{%
\subsubsection{\texorpdfstring{Operational Definition of Confidence
(\(C\))}{Operational Definition of Confidence (C)}}\label{operational-definition-of-confidence-c}}

To fully characterize the phenomenological experience of metacognition,
we additionally measure \textbf{subjective confidence} \(C\).

\textbf{Definition:}

Confidence \(C\) is a \textbf{phenomenological self-report} of
subjective certainty, measured on a scale (e.g., 0-100\% or 1-7 Likert).

\[C(x) \in [0, 1] \quad \text{(or any bounded interval)}\]

\textbf{Key Distinction: K vs C}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Dimension & K (Epistemic State) & C (Confidence) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{What it measures} & Alignment with reference & Subjective
feeling \\
\textbf{Anchor} & Correct/Incorrect/Absent & Certain/Uncertain \\
\textbf{Sign} & Signed (-1 to 1) & Unsigned (0 to 1) \\
\textbf{Basis} & External validation & Internal experience \\
\end{longtable}

\textbf{Orthogonality:}

\(K\) and \(C\) are \textbf{conceptually orthogonal}:

\begin{longtable}[]{@{}lccl@{}}
\toprule\noalign{}
Pattern & \(K_0\) & \(C\) & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Confident correct & 1 & High & Ideal \\
Confident wrong & -1 & High & Dangerous misconception \\
Unconfident correct & 1 & Low & Imposter-like \\
Unconfident wrong & -1 & Low & Appropriate uncertainty \\
\end{longtable}

\textbf{Diagnostic Role:}

\(C\) helps distinguish subtypes within the same \(K\) pattern: - DK
(\(K_0=0\), \(K_1=-1\)) with \textbf{high \(C\)} -\textgreater{}
Overconfident ignorance - DK (\(K_0=0\), \(K_1=-1\)) with \textbf{low
\(C\)} -\textgreater{} Uncertain but still wrong claim

\textbf{K-C Dissociation Hypothesis:}

Subjects can have: - High \(K_0\) with low \(C\) (Imposter syndrome) -
Low \(K_0\) with high \(C\) (Dunning-Kruger effect)

This dissociation is empirically testable and clinically meaningful.

\textbf{Measurement Protocol for C:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Elicit response -\textgreater{} compute \(K_0\)
\item
  Elicit confidence (0-100\%) -\textgreater{} record \(C\)
\item
  Elicit metacognitive claim (``Do you know?'') -\textgreater{} compute
  \(K_1\)
\item
  Analyze \(K \times C\) jointly for full characterization
\end{enumerate}

\hypertarget{claim-elicitation-protocol}{%
\subsubsection{Claim Elicitation
Protocol}\label{claim-elicitation-protocol}}

This section specifies the empirical protocol for eliciting Claim\(_1\)
and Claim\(_2\), addressing the question: ``How do we obtain the claims
needed to compute \(K_1\) and \(K_2\)?''

\hypertarget{standard-protocol-categorical}{%
\paragraph{Standard Protocol
(Categorical)}\label{standard-protocol-categorical}}

\textbf{Claim\(_1\) Elicitation:}

After each response, present:

\begin{quote}
``How confident are you in your answer?'' - {[} {]} I'm confident I'm
correct (``I know'') - {[} {]} I'm not sure (``Not sure'') - {[} {]} I
think I might be wrong (``I don't know'')
\end{quote}

\textbf{Claim\(_2\) Elicitation:}

After completing all items (or after \(K_1\) is computed), present:

\begin{quote}
``How accurate do you think your self-assessments were overall?'' - {[}
{]} My self-assessments were accurate (``Meta-aligned'') - {[} {]} I'm
not sure about my self-assessments (``Meta-uncertain'') - {[} {]} My
self-assessments were probably inaccurate (``Meta-misaligned'')
\end{quote}

\hypertarget{continuous-protocol-slider}{%
\paragraph{Continuous Protocol
(Slider)}\label{continuous-protocol-slider}}

\textbf{Claim\(_1\) Elicitation:}

\begin{quote}
``How confident are you in your answer?'' (slider: 0-100)
\end{quote}

\textbf{Threshold Mapping:}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Slider Value (\(c\)) & Claim\(_1\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(c \geq 70\) & ``I know'' \\
\(30 < c < 70\) & ``Not sure'' \\
\(c \leq 30\) & ``I don't know'' \\
\end{longtable}

\textbf{Threshold Calibration:}

Thresholds should be validated via: 1. \textbf{Pilot calibration}:
Adjust thresholds to achieve approximately equal category frequencies 2.
\textbf{Cross-participant comparison}: Use percentile-based thresholds
(e.g., top 30\%, middle 40\%, bottom 30\%) 3. \textbf{Domain-specific
adjustment}: Higher thresholds for domains with inflated confidence
norms

\hypertarget{inter-rater-reliability-requirements}{%
\paragraph{Inter-Rater Reliability
Requirements}\label{inter-rater-reliability-requirements}}

For categorical claims: - Expected Cohen's \(\kappa > 0.8\) for
Claim\(_1\) coding - Any ambiguous responses should be coded by 2+
independent raters

For continuous claims: - Report ICC (Intraclass Correlation) for slider
reliability - Expected ICC \(> 0.7\) for adequate reliability

\textbf{Protocol Selection Guidance:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2727}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4773}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Study Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Recommended Protocol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Rationale
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Large-scale survey & Categorical & Faster administration, clearer
coding \\
Individual assessment & Continuous & Finer gradation, calibration
analysis \\
Clinical application & Categorical + Continuous & Both for robustness \\
\end{longtable}

\textbf{Important Distinction (Summary):} - \(K(x)\): Epistemic state
(how accurately the subject recognizes \(x\)) - \(C\): Phenomenological
confidence (how certain the subject feels)

These are \textbf{orthogonal dimensions}. A subject can have: -
\(K(x) = 0\) (ignorance) with \(C = 1\) (high confidence) ---
Dunning-Kruger - \(K(x) = 1\) (knowledge) with \(C = 0.5\) (moderate
confidence) --- Underconfidence

\hypertarget{the-c-k-joint-model-empirical-handling-of-confidence-knowledge-interaction}{%
\subsubsection{The C-K Joint Model: Empirical Handling of
Confidence-Knowledge
Interaction}\label{the-c-k-joint-model-empirical-handling-of-confidence-knowledge-interaction}}

\textbf{The Problem:}

Confidence (\(C\)) and epistemic state (\(K\)) are conceptually
orthogonal, but empirically correlated. How should we handle cases like:
- \textbf{Dunning-Kruger}: \(K_1 = -1\) (miscalibrated) with
\(C = \text{high}\) - \textbf{Imposter Syndrome}: \(K_1 = -1\)
(miscalibrated) with \(C = \text{low}\)

\textbf{Proposed Joint Model:}

We model the joint distribution of \((K_n, C)\) as a bivariate
structure:

\[P(K_n, C) = P(K_n) \cdot P(C \mid K_n)\]

where: - \(P(K_n)\): Marginal distribution of epistemic alignment (from
MAT protocol) - \(P(C \mid K_n)\): Conditional distribution of
confidence given alignment

\textbf{Operationalization:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1591}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4773}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3636}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\(K_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expected \(C\) Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(+1\) (calibrated) & \(C\) correlates with \(K_0\) & Confidence tracks
knowledge \\
\(0\) (uncertain) & \(C\) near midpoint & Appropriate uncertainty \\
\(-1\) (miscalibrated) & \(C\) anti-correlates with \(K_0\) & Confidence
misleads \\
\end{longtable}

\textbf{Dunning-Kruger vs Imposter Analysis:}

For subjects with \(K_1 = -1\) (miscalibrated metacognition):

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1765}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1373}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0980}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3137}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2745}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Subtype
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(C\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Intervention
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Dunning-Kruger} & \(0\) or \(-1\) & High & Overconfident
ignorance & Calibration training \\
\textbf{Imposter Syndrome} & \(+1\) & Low & Underconfident knowledge &
Confidence building \\
\end{longtable}

\textbf{Joint Reporting:}

Report \((K_0, K_1, K_2, C)\) as a 4-tuple for complete metacognitive
characterization:

\begin{longtable}[]{@{}lllll@{}}
\toprule\noalign{}
\(K_0\) & \(K_1\) & \(K_2\) & \(C\) & Pattern Name \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(0\) & \(-1\) & \(-1\) & High & Deep Dunning-Kruger (overconfident) \\
\(0\) & \(-1\) & \(+1\) & Low & Aware Dunning-Kruger
(self-correcting) \\
\(+1\) & \(-1\) & \(-1\) & Low & Deep Imposter (persistent
self-doubt) \\
\(+1\) & \(-1\) & \(+1\) & Low & Aware Imposter (recognizes pattern) \\
\end{longtable}

\textbf{Why \(C\) Cannot Replace \(K\):}

While \(C\) and \(K_1\) are empirically correlated, they measure
different things: - \(K_1\) = \textbf{Structural alignment} (does claim
match state?) - \(C\) = \textbf{Phenomenological intensity} (how certain
does subject feel?)

A subject with \(K_1 = +1\) (accurate self-assessment) may have
\(C = 0.3\) (low confidence) --- they correctly identified their state
but did not feel certain about it. The \((K, C)\) joint model captures
this distinction.

\hypertarget{measurement-protocol}{%
\subsubsection{Measurement Protocol}\label{measurement-protocol}}

\hypertarget{step-1-establish-reference-context}{%
\paragraph{Step 1: Establish Reference
Context}\label{step-1-establish-reference-context}}

For each proposition \(x\), establish what counts as ``aligned''
(\(K(x) = 1\)) via the experimental context (e.g., expert consensus,
empirical measurement, community agreement). The proposition \(x\)
itself serves as the implicit reference point.

\hypertarget{step-2-measure-kx-via-task-performance}{%
\paragraph{\texorpdfstring{Step 2: Measure \(K(x)\) via Task
Performance}{Step 2: Measure K(x) via Task Performance}}\label{step-2-measure-kx-via-task-performance}}

\textbf{Task:} Subject answers: ``Is proposition \(x\) true, false, or
unknown?''

\begin{longtable}[]{@{}lcc@{}}
\toprule\noalign{}
Subject's Answer & Reference & Inferred \(K(x)\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``True'' & Aligned & \(1\) (correct knowledge) \\
``False'' & Aligned (proposition is false) & \(1\) (correct
knowledge) \\
``I don't know'' & any & \(0\) (ignorance) \\
``True'' & Opposed & \(-1\) (misconception) \\
``False'' & Opposed (proposition is true) & \(-1\) (misconception) \\
\end{longtable}

\hypertarget{step-3-measure-confidence-c_0}{%
\paragraph{\texorpdfstring{Step 3: Measure Confidence
\(C_0\)}{Step 3: Measure Confidence C\_0}}\label{step-3-measure-confidence-c_0}}

\textbf{Question:} ``On a scale from 0 to 1, how confident are you in
your answer?''

This captures the phenomenological dimension of certainty.

\hypertarget{step-4-elicit-metacognitive-claim}{%
\paragraph{Step 4: Elicit Metacognitive
Claim}\label{step-4-elicit-metacognitive-claim}}

\textbf{Question:} ``Do you know the answer to the previous question?''

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Subject's Claim & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
``Yes, I know'' & Subject claims \(K(K(x)) = 1\) \\
``No, I don't know'' & Subject claims \(K(K(x)) = 0\) \\
``I'm not sure'' & Subject claims \(K(K(x)) \approx 0.5\) \\
\end{longtable}

\hypertarget{step-5-infer-actual-kkx-via-comparison}{%
\paragraph{\texorpdfstring{Step 5: Infer Actual \(K(K(x))\) via
Comparison}{Step 5: Infer Actual K(K(x)) via Comparison}}\label{step-5-infer-actual-kkx-via-comparison}}

Compare the subject's \textbf{metacognitive claim} (Step 4) to their
\textbf{actual state} (Step 2):

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2778}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2222}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Actual \(K(x)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Subject's Claim
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Inferred \(K(K(x))\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Classification
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(1\) (knows) & ``I know'' & \(1\) & \textbf{Knowing Knowledge} \\
\(0\) (ignorant) & ``I don't know'' & \(1\) & \textbf{Knowing Ignorance}
(Socratic) \\
\(0\) (ignorant) & ``I know'' & \(-1\) & \textbf{Unknowing Ignorance}
(Dunning-Kruger) \\
\(1\) (knows) & ``I don't know'' & \(-1\) or \(0\) & \textbf{Unknowing
Knowledge} (Imposter) \\
\end{longtable}

\textbf{Key Insight:} \(K(K(x))\) is inferred by checking whether the
subject's \textbf{metacognitive claim matches their actual state}.

\hypertarget{analyzing-discrepancies}{%
\subsubsection{Analyzing Discrepancies}\label{analyzing-discrepancies}}

\hypertarget{metacognitive-discrepancy}{%
\paragraph{Metacognitive Discrepancy}\label{metacognitive-discrepancy}}

The discrepancy between actual state and metacognitive claim is captured
directly by \(K(K(x))\): - \(K(K(x)) = 1\): Accurate metacognition
(claim matches reality) - \(K(K(x)) = 0\): Partial metacognitive failure
- \(K(K(x)) = -1\): Complete metacognitive failure (claim contradicts
reality)

\hypertarget{experimental-design-the-metacognitive-alignment-test-mat}{%
\subsection{Experimental Design: The Metacognitive Alignment Test
(MAT)}\label{experimental-design-the-metacognitive-alignment-test-mat}}

To demonstrate the falsifiability and measurability of this model, we
propose the \textbf{Metacognitive Alignment Test (MAT)}.

\hypertarget{objectives}{%
\subsubsection{Objectives}\label{objectives}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Measure \(K(x)\) (first-order epistemic state)
\item
  Measure \(K(K(x))\) (second-order metacognitive state)
\item
  Measure confidence \(C\) (phenomenological dimension)
\item
  Validate the distinction between Socratic Wisdom and Dunning-Kruger
  effect
\end{enumerate}

\hypertarget{protocol}{%
\subsubsection{Protocol}\label{protocol}}

\textbf{Phase 1: Knowledge Assessment} - Present factual questions with
established reference answers (e.g., expert consensus) - Subject
responds: True / False / I don't know - Calculate \(K(x)\) based on
alignment with reference

\textbf{Phase 2: Confidence Rating} - Subject rates confidence: ``How
confident are you?'' (0-1 scale) - Record \(C_0\)

\textbf{Phase 3: Metacognitive Claim} - Ask: ``Do you know the answer to
the previous question?'' - Subject responds: Yes / No / Unsure - Infer
\(K(K(x))\) by comparing claim to actual \(K(x)\)

\textbf{Phase 4: Validation Tasks} - Present decision-making scenarios
requiring self-assessment - Measure performance on tasks like: -
Deciding when to seek help - Allocating study time - Deferring to
experts

\hypertarget{validation-hypothesis}{%
\subsubsection{Validation Hypothesis}\label{validation-hypothesis}}

\textbf{Hypothesis:} Subjects with high \(K(K(x))\) (accurate
metacognition) will perform better on validation tasks,
\textbf{regardless of their raw \(K(x)\) score}.

This would validate the model's claim that: - \textbf{Knowing Ignorance}
(\(K(x) = 0, K(K(x)) = 1\)) is a valuable cognitive state -
Metacognitive accuracy is distinct from first-order knowledge - Socratic
wisdom has measurable benefits

\hypertarget{expected-patterns}{%
\subsubsection{Expected Patterns}\label{expected-patterns}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1739}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1739}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(K(x)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(K(K(x))\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(C\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Expected Behavior
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Socratic Wisdom & \(0\) & \(1\) & Low & Seeks help appropriately \\
Dunning-Kruger & \(0\) & \(-1\) & High & Overconfident errors \\
Accurate Expert & \(1\) & \(1\) & High & Confident and correct \\
Imposter Syndrome & \(1\) & \(-1\) or \(0\) & Low & Underconfident but
correct \\
\end{longtable}

\hypertarget{relationship-to-established-metrics}{%
\subsubsection{Relationship to Established
Metrics}\label{relationship-to-established-metrics}}

The MAT is designed to \textbf{complement, not replace}, existing
metacognitive measures:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What it measures
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relationship to MAT
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{meta-d'} & Metacognitive sensitivity (discrimination) & Can be
computed from MAT data; provides aggregate validation \\
\textbf{Brier Score} & Probabilistic calibration & Applicable if
confidence is elicited as probability \\
\textbf{ECE} & Expected calibration error & Measures bias in
confidence-accuracy relationship \\
\textbf{AUROC} & Discrimination ability & Can be derived from confidence
ratings vs.~accuracy \\
\textbf{IRT} & Item difficulty and discrimination & Can model item-level
variance in MAT responses \\
\end{longtable}

\textbf{Recommended Analysis Pipeline:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \(K(x)\) and \(K(K(x))\) per item using MAT protocol
\item
  Compute meta-d' across trials as aggregate metacognitive sensitivity
\item
  Compute calibration metrics (Brier, ECE) from confidence ratings
\item
  Compare \(K(K(x))\) patterns (Socratic, Dunning-Kruger, etc.) with
  meta-d' to validate convergent validity
\item
  Use IRT to account for item-level heterogeneity
\end{enumerate}

\textbf{Hypothesis:} High \(K(K(x))\) (accurate metacognition) should
correlate with high meta-d'/d' ratio and good calibration, but
\(K(K(x))\) provides additional structural information (e.g.,
distinguishing Socratic wisdom from mere low confidence).

\hypertarget{related-work}{%
\subsection{Related Work}\label{related-work}}

\hypertarget{relationship-to-modal-epistemic-logic}{%
\subsubsection{Relationship to Modal Epistemic
Logic}\label{relationship-to-modal-epistemic-logic}}

\hypertarget{background-knowledge-operators-in-modal-logic}{%
\paragraph{Background: Knowledge Operators in Modal
Logic}\label{background-knowledge-operators-in-modal-logic}}

In formal epistemology, the knowledge operator \(\mathbf{K}\) is studied
within modal logic frameworks (Hintikka, 1962). Key systems include:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
System
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Axioms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{K} & Distribution:
\(\mathbf{K}(p \to q) \to (\mathbf{K}p \to \mathbf{K}q)\) & Basic
epistemic closure \\
\textbf{T} & Veridicality: \(\mathbf{K}p \to p\) & Knowledge implies
truth \\
\textbf{S4} & Positive introspection: \(\mathbf{K}p \to \mathbf{KK}p\) &
If I know, I know that I know \\
\textbf{S5} & Negative introspection:
\(\neg\mathbf{K}p \to \mathbf{K}\neg\mathbf{K}p\) & If I don't know, I
know that I don't know \\
\end{longtable}

\hypertarget{correspondence-with-k-framework}{%
\paragraph{Correspondence with K
Framework}\label{correspondence-with-k-framework}}

Our \(K_n\) framework can be positioned relative to these axioms:

\textbf{Axiom T (Veridicality)}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2353}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Modal Logic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
K Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Status
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mathbf{K}p \to p\) & \(K_0 = +1 \Rightarrow\) Reference = correct &
\textbf{Definitionally satisfied} \\
\end{longtable}

By construction, \(K_0 = +1\) requires alignment with the reference, so
veridicality is built into the scoring.

\textbf{Axiom S4 (Positive Introspection)}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2353}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Modal Logic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
K Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Status
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\mathbf{K}p \to \mathbf{KK}p\) & \(K_0 = +1 \Rightarrow K_1 = +1\)
(idealized) & \textbf{Empirically violated} \\
\end{longtable}

S4 describes an \emph{ideal} agent. Real agents may have \(K_0 = +1\)
but \(K_1 = -1\) (Impostor Syndrome pattern). The K framework
\emph{measures} such violations rather than assuming them away.

\textbf{Axiom S5 (Negative Introspection)}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3824}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2353}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Modal Logic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
K Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Status
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\neg\mathbf{K}p \to \mathbf{K}\neg\mathbf{K}p\) &
\(K_0 \leq 0 \Rightarrow K_1 = +1\) (idealized) & \textbf{Empirically
violated} \\
\end{longtable}

S5 describes \emph{ideal} self-awareness. Real agents may have
\(K_0 = -1\) but \(K_1 = -1\) (Dunning-Kruger pattern). Again, the K
framework measures rather than assumes.

\hypertarget{key-distinction-normative-vs-descriptive}{%
\paragraph{Key Distinction: Normative vs
Descriptive}\label{key-distinction-normative-vs-descriptive}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2826}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Approach
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Modal Epistemic Logic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
K Framework
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Stance} & Normative (ideal agent) & Descriptive (empirical
measurement) \\
\textbf{Axioms} & Prescribe what \emph{should} hold & Diagnose what
\emph{does} hold \\
\textbf{Violations} & Indicate irrationality & Indicate metacognitive
failure modes \\
\textbf{Purpose} & Reasoning about ideal knowledge & Measuring actual
metacognition \\
\end{longtable}

\textbf{Complementarity}:

Modal logic provides the \emph{normative benchmark} against which
metacognitive accuracy can be evaluated. The K framework provides the
\emph{measurement apparatus} to assess how far real agents deviate from
this benchmark.

\[\text{Metacognitive gap} = \text{Ideal (S4/S5)} - \text{Actual (K}_1\text{)}\]

\hypertarget{summary-table-axiom-correspondence}{%
\paragraph{Summary Table: Axiom
Correspondence}\label{summary-table-axiom-correspondence}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1224}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4490}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Axiom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
S5 Statement
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our Framework Status
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{T} & Truth & \(\mathbf{K}\phi \to \phi\) & Satisfied:
\(K_0 = +1\) implies correctness \\
\textbf{4} & Positive Introspection &
\(\mathbf{K}\phi \to \mathbf{KK}\phi\) & \textbf{Violated}: \(K_0 = +1\)
does NOT imply \(K_1 = +1\) \\
\textbf{5} & Negative Introspection &
\(\neg\mathbf{K}\phi \to \mathbf{K}\neg\mathbf{K}\phi\) &
\textbf{Violated}: \(K_0 = 0\) does NOT imply \(K_1 = +1\) \\
\end{longtable}

\textbf{Key Departure}:

Epistemic logics model \textbf{what agents should know} under idealized
conditions. Our framework models \textbf{what agents actually exhibit}
under empirical observation, including systematic failures of
introspection.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1778}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4889}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Epistemic Logic (S5)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our Framework
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Agents} & Idealized, logically consistent & Empirical,
cognitively fallible \\
\textbf{Introspection} & Perfect (axioms 4, 5 hold) & Imperfect
(Dunning-Kruger, Imposter) \\
\textbf{Semantics} & Modal (possible worlds) & Observational
(state-based measurement) \\
\textbf{Purpose} & Normative reasoning about ideal agents & Descriptive
measurement of real agents \\
\textbf{Misconception} & Not modeled (\(\mathbf{K}\phi\) only for true
\(\phi\)) & Explicitly modeled (\(K = -1\)) \\
\end{longtable}

\textbf{Complementary Use}: Epistemic logic provides normative
benchmarks (what perfect metacognition would look like); our framework
measures deviations from those benchmarks in real agents. The ``axiom
violations'' we observe are precisely the phenomena of interest.

\hypertarget{kripke-semantics-interpretation}{%
\subsubsection{Kripke Semantics
Interpretation}\label{kripke-semantics-interpretation}}

\hypertarget{background}{%
\paragraph{Background}\label{background}}

In Kripke semantics, knowledge is modeled via accessibility relations
over possible worlds:

\[\mathbf{K}p \text{ holds at world } w \iff p \text{ holds at all worlds accessible from } w\]

\hypertarget{correspondence-with-k-framework-1}{%
\paragraph{Correspondence with K
Framework}\label{correspondence-with-k-framework-1}}

\textbf{State Spaces as World Sets}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2766}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3830}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3404}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
K Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kripke Semantics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\text{State}_0\) & Actual world \(w_0\) & Ground truth state \\
\(\text{State}_1\) & Epistemic accessibility from \(w_0\) & Worlds
consistent with agent's beliefs about \(w_0\) \\
\(\text{State}_2\) & Meta-accessibility & Worlds consistent with agent's
beliefs about \(\text{State}_1\) \\
\end{longtable}

\textbf{K Values as Accessibility Measures}:

\[K_n = \begin{cases}
+1 & \text{All accessible worlds agree (certain knowledge)} \\
0 & \text{Accessible worlds are indeterminate (ignorance)} \\
-1 & \text{All accessible worlds disagree with actual (misconception)}
\end{cases}\]

\textbf{Formalization}:

Let \(R_n\) be the accessibility relation at layer \(n\). Then:

\[K_n = \frac{|\{w' : w R_n w' \land \text{State}_n(w') = \text{State}_n(w_0)\}| - |\{w' : w R_n w' \land \text{State}_n(w') \neq \text{State}_n(w_0)\}|}{|\{w' : w R_n w'\}|}\]

This yields \(K_n = +1\) when all accessible worlds match actuality,
\(K_n = -1\) when none do.

\textbf{Note}: This proportion-based formulation is a \textbf{proposed
operationalization} extending standard Kripke semantics (which is
binary: know/don't know) to a graded scale. This extension is novel to
our framework and should be understood as an empirical approximation
rather than a direct entailment from modal logic.

\hypertarget{limitations-of-the-correspondence}{%
\paragraph{Limitations of the
Correspondence}\label{limitations-of-the-correspondence}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Finite vs Infinite}: Kripke models allow infinite world sets;
  K framework assumes finite enumeration via responses/claims
\item
  \textbf{Quantitative vs Qualitative}: K provides graded values;
  standard Kripke is binary (know/don't know)
\item
  \textbf{Observational vs Semantic}: K is defined operationally via
  behavior; Kripke is defined model-theoretically
\end{enumerate}

\textbf{Conclusion}: The K framework can be viewed as an \emph{empirical
operationalization} of Kripke-style accessibility, where accessibility
is inferred from behavioral responses rather than stipulated
semantically.

\hypertarget{connection-to-dynamic-epistemic-logic-del}{%
\subsubsection{Connection to Dynamic Epistemic Logic
(DEL)}\label{connection-to-dynamic-epistemic-logic-del}}

\textbf{Connection to Dynamic Epistemic Logic (DEL):}

DEL models knowledge change through public announcements and private
observations. Our framework can be seen as \textbf{static snapshots}
within a DEL-like dynamics:

\begin{itemize}
\tightlist
\item
  \(K_n\) at time \(t\) = epistemic state after observation sequence
\item
  Intervention = action that changes the model
\item
  Re-observation = new \(K_n\) measurement
\end{itemize}

Full integration with DEL (multi-agent, announcement operators) is
beyond current scope but represents a natural extension.

\hypertarget{relationship-to-graded-epistemic-logics}{%
\subsubsection{Relationship to Graded Epistemic
Logics}\label{relationship-to-graded-epistemic-logics}}

Recent work in graded epistemic logics (e.g., S5G frameworks) models
knowledge with continuous plausibility values in \([0, 1]\). Our
framework differs in key respects:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Graded Epistemic Logics
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our \(K(K(x))\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Scale} & \([0, 1]\) (plausibility) & \([-1, 1]\) (includes
misconception) \\
\textbf{Misconception} & Typically not modeled & \(K(x) = -1\) \\
\textbf{Metacognition} & Introspection axioms & Explicit recursive
operator \\
\textbf{Focus} & Idealized agents & Psychologically realistic
failures \\
\end{longtable}

\textbf{Formal Correspondence:}

Our \(K\) can be viewed as a \textbf{graded, psychologically realistic}
extension that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Relaxes positive/negative introspection axioms to accommodate
  metacognitive failures
\item
  Extends the scale to include misconception (negative values)
\item
  Focuses on the \textbf{gap} between actual and recognized epistemic
  states, rather than idealized consistency
\end{enumerate}

Whether \(K\) is idempotent (\(K(K(x)) = K(x)\) for accurate
metacognizers) or contractive (higher-order reflection converges) is an
\textbf{empirical question} that our framework can accommodate but does
not presuppose.

\hypertarget{correspondence-with-established-metrics-1}{%
\subsubsection{Correspondence with Established
Metrics}\label{correspondence-with-established-metrics-1}}

Before detailed comparisons, we provide a summary of how the \(K\)
framework relates to established metrics at each layer.

\hypertarget{layer-by-layer-mapping}{%
\paragraph{Layer-by-Layer Mapping}\label{layer-by-layer-mapping}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2321}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3571}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2857}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
K Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Established Metric
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Correspondence
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(K_0\)} & Epistemic state & IRT ability \(\theta\) &
\(K_0 \approx 2\Phi(\theta) - 1\) (scaled) \\
\textbf{\(K_0\)} & Epistemic state & Accuracy & \(K_0 = +1\) iff
correct \\
\textbf{\(K_1\)} & Metacognitive alignment & meta-d' &
\(K_1 \propto \text{meta-}d' / d'\) (normalized sensitivity) \\
\textbf{\(K_1\)} & Metacognitive alignment & AUC (Type-2) &
\(K_1 = 2 \cdot \text{AUC} - 1\) \\
\textbf{\(K_1\)} & Metacognitive alignment & Calibration (ECE) &
\(K_1 \approx 1 - 2 \cdot \text{ECE}\) (inverse relationship) \\
\textbf{\(K_2\)} & Meta-metacognitive alignment & Stability of \(K_1\)
across contexts & Novel measure \\
\end{longtable}

\hypertarget{detailed-correspondences}{%
\paragraph{Detailed Correspondences}\label{detailed-correspondences}}

\textbf{\(K_1\) vs meta-d':}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3810}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1905}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
meta-d'
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Scale} & \([-1, +1]\) & \((-\infty, +\infty)\) \\
\textbf{Zero point} & No systematic alignment & Chance-level
metacognition \\
\textbf{Aggregation} & Per-item, then averaged & Aggregate over item
set \\
\textbf{Assumptions} & Minimal (monotonicity) & SDT model (Gaussian
distributions) \\
\end{longtable}

\textbf{Approximate Translation:}
\[K_1 \approx \tanh\left(\frac{\text{meta-}d'}{2}\right)\]

\textbf{\(K_1\) vs Calibration Error (ECE):}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4706}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2353}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2941}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ECE
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Direction} & +1 = perfect alignment & 0 = perfect calibration \\
\textbf{Meaning of negative} & Systematic misalignment & N/A (always
\(\geq\) 0) \\
\textbf{Confidence dimension} & Implicit in State\(_1\) & Explicit
(confidence bins) \\
\end{longtable}

\textbf{Note:} \(K_1\) captures \emph{direction} of miscalibration
(over- vs under-confidence), while ECE captures \emph{magnitude} only.
They are complementary, not redundant.

\hypertarget{joint-reporting-standard-proposed}{%
\paragraph{Joint Reporting Standard
(Proposed)}\label{joint-reporting-standard-proposed}}

For comprehensive metacognitive assessment, we recommend reporting:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2308}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2051}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5641}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Measure
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Information Captured
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(K_0\)} & This framework & Epistemic state
(knowledge/ignorance/misconception) \\
\textbf{\(K_1\)} & This framework & Alignment direction and magnitude \\
\textbf{\(K_2\)} & This framework & Meta-metacognitive calibration \\
\textbf{meta-d'} & SDT & Aggregate sensitivity under SDT assumptions \\
\textbf{ECE} & Calibration & Confidence-accuracy gap magnitude \\
\textbf{Brier} & Calibration & Combined accuracy + calibration \\
\end{longtable}

This joint profile provides a complete picture: \(K_n\) for item-level
structure, aggregate metrics for overall performance.

\hypertarget{positioning-among-related-frameworks}{%
\subsubsection{Positioning Among Related
Frameworks}\label{positioning-among-related-frameworks}}

Before comparing specific metrics, we clarify the \textbf{role of the
\(K\) framework} relative to existing approaches:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Role
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relationship to \(K\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{meta-d'} (Maniscalco \& Lau) & Aggregate sensitivity metric &
\(K\) provides per-item classification \\
\textbf{meta-I} (Dayan, 2023) & Information-theoretic sensitivity &
\(K\) adds direction and recursion \\
\textbf{HiBayES} (Fleming \& Daw, 2017) & Hierarchical estimation engine
& \(K\) defines what to estimate \\
\textbf{IRT} (psychometrics) & Latent trait estimation & \(K_0\) uses
IRT as estimation engine \\
\textbf{Calibration metrics} (Brier, ECE) & Confidence-accuracy
alignment & \(K\) adds structural classification \\
\end{longtable}

\textbf{Key Distinction}:

These frameworks address \textbf{how well} metacognition works
(sensitivity, efficiency). The \(K\) framework addresses \textbf{what
type} of metacognitive state exists (classification, coordinates).

\textbf{Analogy}: - meta-d' / meta-I = \textbf{Thermometer} (measures
metacognitive temperature) - \(K\) = \textbf{Weather map} (classifies
patterns, guides intervention)

\textbf{Integration Potential}:

The \(K\) framework serves as a \textbf{common coordinate system} that
unifies: - Behavioral assessments (response-claim alignment) -
Signal-detection metrics (meta-d', AUROC) - Hierarchical estimation
(HiBayES) - Calibration analysis (Brier, ECE)

This is not ``reinventing the wheel'' but \textbf{designing the axle}
that connects existing wheels.

\hypertarget{metacognitive-sensitivity-meta-d}{%
\subsubsection{Metacognitive Sensitivity:
meta-d'}\label{metacognitive-sensitivity-meta-d}}

Maniscalco and Lau (2012) developed the \emph{meta-d'} framework for
measuring metacognitive sensitivity---the ability to discriminate
between correct and incorrect responses via confidence ratings.

\textbf{Formal Correspondence with Type-2 SDT:}

In Type-2 SDT, meta-d' is the d' that would produce the observed Type-2
ROC if the observer had optimal metacognitive access to their Type-1
evidence.

\textbf{Definition (meta-d'):}

\[\text{meta-d}' = \Phi^{-1}(\text{HR}_2) - \Phi^{-1}(\text{FAR}_2)\]

Where: - \(\text{HR}_2\) = Type-2 Hit Rate =
\(P(\text{high confidence} | \text{correct})\) - \(\text{FAR}_2\) =
Type-2 False Alarm Rate =
\(P(\text{high confidence} | \text{incorrect})\) - \(\Phi^{-1}\) =
inverse standard normal CDF

\textbf{Mapping \(K_1\) to meta-d':}

\[K_1 = \tanh\left(\frac{\text{meta-d}'}{2}\right)\]

\textbf{Derivation:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  meta-d' \(\in (-\infty, +\infty)\), with 0 = chance, positive =
  above-chance sensitivity
\item
  \(\tanh\) maps \((-\infty, +\infty) \to (-1, +1)\) monotonically
\item
  The factor of 2 ensures that meta-d' \(= 2\) (good sensitivity) maps
  to \(K_1 \approx 0.76\)
\end{enumerate}

\textbf{M-Ratio Alternative:}

\[K_1 = \text{M-ratio} - 1 = \frac{\text{meta-d}'}{d'} - 1\]

Where M-ratio = 1 indicates ideal metacognitive efficiency (perfect
metacognitive access).

\textbf{Complete Correspondence Table:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3256}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3488}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3256}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
SDT Quantity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\(K\) Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Relationship
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
d' (Type-1) & Related to \(K_0\) &
\(d' \approx 2 \cdot \text{arctanh}(K_0)\) \\
Type-2 HR & \(P(\text{"I know"} | K_0 = +1)\) & Direct
operationalization \\
Type-2 FAR & \(P(\text{"I know"} | K_0 \leq 0)\) & Direct
operationalization \\
meta-d' & Related to \(K_1\) &
\(K_1 \approx \tanh(\text{meta-d}'/2)\) \\
M-ratio & Metacognitive efficiency & \(K_1 \approx \text{M-ratio} - 1\)
(if \(d' = 2\)) \\
Type-2 AUROC & Discrimination accuracy &
\(K_1 \approx 2 \cdot \text{AUROC} - 1\) \\
\end{longtable}

\textbf{Important Note:} These are approximate correspondences valid
under specific assumptions (symmetric criteria, Gaussian noise). Exact
relationships depend on model parameters and should be validated
empirically.

\textbf{What \(K\) Adds Beyond meta-d'}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Signed direction}: meta-d' is unsigned; \(K_1\) distinguishes
  overconfidence (\(-1\)) from underconfidence
\item
  \textbf{Per-item granularity}: meta-d' is aggregate; \(K_1\) can be
  computed per item
\item
  \textbf{Explicit ignorance}: ``I don't know'' is modeled as
  \(K_0 = 0\), not low confidence
\item
  \textbf{Recursive hierarchy}: \(K_2, K_3, \ldots\) extend beyond
  Type-2
\end{enumerate}

\textbf{Comparison:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
meta-d'
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our \(K(K(x))\)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Focus} & Discrimination ability (sensitivity) & Structural
accuracy (recognition) \\
\textbf{Measurement} & Statistical correlation across trials & Per-item
metacognitive state \\
\textbf{Granularity} & Aggregate across trials & Per-item \\
\textbf{``I don't know''} & Treated as low confidence &
\(K(x)=0, K(K(x))=1\) (Socratic wisdom) \\
\textbf{Statistical Model} & Noise-tolerant (SDT) & Deterministic
match/mismatch \\
\textbf{Theoretical Basis} & Signal Detection Theory & Recursive
epistemology \\
\end{longtable}

\textbf{Key Difference:} meta-d' measures whether confidence ratings
\textbf{correlate} with accuracy. Our model measures whether
metacognitive claims \textbf{match} actual states. Crucially, we
recognize that \textbf{accurately knowing one's ignorance}
(\(K(x) = 0, K(K(x)) = 1\)) is a \textbf{high metacognitive
achievement}, not a failure.

\textbf{Complementary Relationship:} These approaches are \textbf{not
mutually exclusive}. meta-d' provides a noise-tolerant aggregate measure
of metacognitive sensitivity; our \(K(K(x))\) provides per-item
structural classification with explicit treatment of Socratic wisdom. An
integrated approach could: - Use meta-d' for aggregate sensitivity
analysis across trials - Use \(K(K(x))\) for per-item classification and
Socratic wisdom detection - Define a continuous version:
\(K(K(x)) = 2 \cdot P(\text{meta-claim matches actual state}) - 1\),
estimated across trials via hierarchical Bayesian methods

\hypertarget{information-theoretic-metacognition-meta-i}{%
\subsubsection{Information-Theoretic Metacognition:
meta-I}\label{information-theoretic-metacognition-meta-i}}

Dayan (2023) introduced \emph{meta-I}, a \textbf{model-free}
information-theoretic measure of metacognitive sensitivity:

\[\text{meta-I} = H(\text{accuracy}) - H(\text{accuracy} | \text{confidence})\]

This measures mutual information between confidence and accuracy,
quantifying how much confidence reduces uncertainty about accuracy.

\textbf{Comparison:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
meta-I
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our \(K\) Framework
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Theoretical Basis} & Information Theory & Recursive
Epistemology \\
\textbf{Model Dependency} & Model-free & Model-free (observational) \\
\textbf{Direction} & Unsigned (sensitivity only) & \textbf{Signed}
(over/under-confidence) \\
\textbf{Layers} & Single layer & \textbf{Recursive}
(\(K_0, K_1, K_2, \ldots\)) \\
\textbf{Output} & Bits (continuous) & \([-1, 1]\) (continuous) \\
\textbf{Granularity} & Can measure response granularity & Per-item
structural classification \\
\end{longtable}

\textbf{Key Distinction:}

Both meta-I and \(K\) are \textbf{model-free} (unlike meta-d' which
requires SDT assumptions), but they serve different purposes:

\begin{itemize}
\tightlist
\item
  \textbf{meta-I} answers: ``How well does confidence track accuracy?''
  (quantitative sensitivity)
\item
  \textbf{\(K\)} answers: ``What type of metacognitive pattern is
  this?'' (qualitative classification)
\end{itemize}

\textbf{Complementary Use Case:}

Two subjects with identical meta-I = 0.3 bits (low sensitivity): -
Subject A: \(K_0=0\), \(K_1=-1\) -\textgreater{} Dunning-Kruger
-\textgreater{} needs awareness intervention - Subject B: \(K_0=1\),
\(K_1=-1\) -\textgreater{} Imposter -\textgreater{} needs
confidence-building

meta-I cannot distinguish these cases; \(K\) can.

\hypertarget{calibration-metrics-brier-score-ece}{%
\subsubsection{Calibration Metrics (Brier Score,
ECE)}\label{calibration-metrics-brier-score-ece}}

Calibration metrics measure whether confidence aligns with accuracy
across many trials.

\textbf{Comparison:}

\begin{longtable}[]{@{}lcl@{}}
\toprule\noalign{}
Aspect & Calibration Metrics & Our \(K(K(x))\) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Granularity} & Aggregate statistics & Individual items \\
\textbf{Purpose} & Probabilistic accuracy & Epistemic state
recognition \\
\textbf{Socratic Wisdom} & Not explicitly modeled & Explicitly
formalized \\
\end{longtable}

Calibration metrics and \(K\) are complementary: calibration measures
aggregate confidence-accuracy alignment, while \(K\) provides per-item
structural classification.

\hypertarget{belief-functions-and-uncertainty-dempster-shafer-theory}{%
\subsubsection{Belief Functions and Uncertainty (Dempster-Shafer
Theory)}\label{belief-functions-and-uncertainty-dempster-shafer-theory}}

Dempster-Shafer theory handles \textbf{epistemic uncertainty} and
\textbf{conflicting evidence} via belief functions.

\textbf{Comparison:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Aspect
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dempster-Shafer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Our Model
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Focus} & Uncertainty quantification & Metacognitive
discrepancy \\
\textbf{Application} & Evidence combination & Self-awareness
structure \\
\textbf{Ignorance} & Represented as belief mass & \(K(x) = 0\)
(epistemic state) \\
\end{longtable}

Dempster-Shafer addresses uncertainty quantification; \(K\) addresses
metacognitive discrepancy (the gap between what one knows and what one
thinks one knows).

\hypertarget{dunning-kruger-effect-empirical-psychology}{%
\subsubsection{Dunning-Kruger Effect (Empirical
Psychology)}\label{dunning-kruger-effect-empirical-psychology}}

Kruger and Dunning (1999) empirically demonstrated that low-competence
individuals overestimate their abilities.

\textbf{Our Contribution:} We provide a \textbf{formal mathematical
model} of this phenomenon: - \(K(x) = 0\) (low competence) -
\(K(K(x)) = -1\) (misrecognition: believes they have competence) - Often
accompanied by \(C = 1\) (high confidence)

This formalization enables: 1. Precise measurement protocols 2.
Distinction from related phenomena (e.g., imposter syndrome) 3.
Extension to arbitrary depths of self-reflection

\hypertarget{application-to-ai-metacognition}{%
\subsubsection{Application to AI
Metacognition}\label{application-to-ai-metacognition}}

The \(K\) framework provides a structured vocabulary for evaluating
metacognition in Large Language Models (LLMs), an increasingly important
area as AI systems are deployed in high-stakes domains.

\textbf{Mapping LLM Behaviors:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1739}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\centering\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2174}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1739}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
LLM Pattern
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(K_0\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(K_1\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(K_2\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Correct + confident & 1 & 1 & 1 & Ideal calibration \\
Hallucination + confident & -1 & -1 & ? & Confident wrong (dangerous) \\
Correct + hedging & 1 & -1 & ? & Underconfident (imposter-like) \\
Admits uncertainty & 0 & 1 & 1 & Appropriate uncertainty (Socratic) \\
``I don't know'' when wrong & -1 & 1 & ? & Partial awareness of
limits \\
\end{longtable}

\textbf{Testbed Proposal:}

Using decoupled confidence elicitation (analogous to AFCE-style
protocols):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Query LLM for answer} -\textgreater{} compute \(K_0\) (against
  ground truth)
\item
  \textbf{Query LLM for confidence} -\textgreater{} record \(C\)
  (0-100\%)
\item
  \textbf{Query LLM: ``Do you know this?''} -\textgreater{} elicit
  \(\text{Claim}_1\)
\item
  \textbf{Compute \(K_1\)} from \(\text{Claim}_1\) vs \(K_0\)
\end{enumerate}

This protocol allows testing: - \textbf{K vs C dissociation} in LLMs (do
they exhibit Dunning-Kruger or Imposter patterns?) -
\textbf{Domain-specific calibration} (are LLMs more self-aware in some
domains?) - \textbf{Intervention effects} (does prompting for
self-reflection improve \(K_1\)?)

\textbf{Why K is Useful for LLM Evaluation:}

Current LLM calibration research focuses primarily on
confidence-accuracy correlation (analogous to meta-d' or meta-I). The
\(K\) framework adds:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Pattern classification}: Identifying \emph{which type} of
  miscalibration
\item
  \textbf{Directional information}: Distinguishing overconfidence from
  underconfidence
\item
  \textbf{Intervention guidance}: Suggesting targeted prompting
  strategies
\end{enumerate}

\textbf{Future Work:} Operationalizing \(K_n\) for LLMs using abstention
behavior, self-consistency checks, and metamorphic testing remains an
open challenge (see Limitations).

\textbf{Layer-wise Operationalization for LLMs:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4464}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4286}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Layer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Human Operationalization
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LLM Operationalization
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{\(K_0\)} & Response vs ground truth & Output vs benchmark
answer \\
\textbf{\(K_1\)} & Self-assessment claim & Verbalized confidence /
hedging \\
\textbf{\(K_2\)} & Meta-self-assessment & ``How reliable is my
confidence?'' prompt \\
\end{longtable}

\hypertarget{connection-to-recent-research}{%
\paragraph{Connection to Recent
Research}\label{connection-to-recent-research}}

\emph{Note: Right-column descriptions are interpretations of these works
within the K framework, not claims made by the original authors.}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3750}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Research Area
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
K Framework Contribution
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Human-AI metacognition} (Fernandes et al., 2024;
arXiv:2409.16708) & \(K_1\) patterns provide a natural way to describe
why the Dunning-Kruger effect disappears with AI assistance; AI
``levels'' metacognitive accuracy \\
\textbf{LLM uncertainty communication} (Steyvers et al., 2025;
arXiv:2510.05126) & Fine-tuning improves LLM calibration/discrimination;
\(K_1\) highlights directional (over/under-confidence) patterns that
scalar metrics like ECE cannot fully characterize \\
\textbf{Latent knowledge probing} (Burns et al., 2022; arXiv:2212.03827,
ICLR 2023) & Probing internal activations reveals what LLMs ``know'' vs
``say''; potential alternative to verbalized \(g_0\) \\
\textbf{LLM metacognitive capacity} (Li et al., 2025; arXiv:2505.13763)
& Neurofeedback paradigm quantifies LLM self-monitoring; suggests
``metacognitive space'' is low-dimensional \\
\textbf{VLM uncertainty estimation} (Lin et al., 2025; arXiv:2511.22019)
& Post-hoc probabilistic embeddings for error detection; one concrete
operational choice for \(g_n/\hat{K}\) in vision-language settings \\
\textbf{Probabilistic VLM embeddings} (Venkataramanan et al., 2025;
arXiv:2505.05163, UAI 2025) & GPLVM-based uncertainty calibration;
aligns with Option B's probabilistic \(K_n\) estimation \\
\textbf{Two-level metacognitive architecture} (Li et al., 2025;
arXiv:2511.23262) & Meta-level/object-level separation mirrors
\(K_0\)/\(K_1\) layer structure; can serve as a testbed for probing
whether meta-reasoning improves \(K_1\)-type behaviour without
necessarily improving \(K_0\) \\
\textbf{Human-AI teaming} & Match human \(K_n\) patterns with AI \(K_n\)
for improved collaboration \\
\end{longtable}

\hypertarget{correspondence-with-monitorgenerateverify-architectures}{%
\paragraph{Correspondence with Monitor--Generate--Verify
Architectures}\label{correspondence-with-monitorgenerateverify-architectures}}

Recent LLM metacognition research formalizes recursive self-monitoring
via Monitor--Generate--Verify (MGV) loops. The \(K\) framework provides
a natural coordinate system for these architectures.

\textbf{Mapping MGV Components to \(K\) Layers:}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
MGV Component & \(K\) Layer & Correspondence \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Generate} & \(K_0\) & First-order response quality \\
\textbf{Monitor} & \(K_1\) & Self-assessment of response quality \\
\textbf{Verify} & \(K_2\) & Meta-assessment of monitoring accuracy \\
\end{longtable}

\textbf{Iteration Dynamics:}

In MGV, the loop proceeds: 1. Generate response → observe \(K_0\)
(against reference) 2. Monitor quality → compute \(K_1\)
(self-assessment alignment) 3. Verify monitoring → compute \(K_2\)
(meta-monitoring accuracy) 4. If \(K_2 < \tau\), regenerate (loop back
to step 1)

\textbf{\(K\) Framework as Stopping Criterion:}

\[\text{Accept output iff } K_1 \geq \tau_1 \text{ AND } K_2 \geq \tau_2\]

This formalizes the intuition that LLMs should only output when: - They
are confident (\(K_1 \geq \tau_1\)), AND - That confidence is justified
(\(K_2 \geq \tau_2\))

\textbf{Complementarity:}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2821}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2564}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4615}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Framework
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Provides
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Does NOT Provide
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
MGV & Process (how to iterate) & Coordinates (where in metacognitive
space) \\
\(K\) Framework & Coordinates (where the system is) & Process (how to
move) \\
\end{longtable}

\textbf{Integration Potential:}

MGV architectures can use \(K_n\) as the objective function for
optimization: - Maximize \(E[K_1 | \text{generation strategy}]\) -
Minimize variance of \(K_2\) across domains

This integration enables principled design of self-improving LLM
systems.

\textbf{Scope Boundary:} Detailed LLM operationalization and experiments
are marked for future work.

\hypertarget{novel-contributions}{%
\subsubsection{Novel Contributions}\label{novel-contributions}}

This study is novel in:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Recursive Formalization}: Extending metacognition to arbitrary
  depths (\(K_0 \to K_1 \to K_2 \to ...\))
\item
  \textbf{Socratic Wisdom as Achievement}: Explicitly modeling ``knowing
  ignorance'' as \(K_0(x)=0, K_1(x)=1\)
\item
  \textbf{Layered Observation Model}: Justifying recursive structure via
  indexed observations with anchor constraints
\item
  \textbf{Orthogonal Dimensions}: Separating epistemic state (\(K\))
  from phenomenological confidence (\(C\))
\item
  \textbf{Per-Item Granularity}: Measuring metacognition at the
  individual item level, not just aggregate statistics
\end{enumerate}

\hypertarget{conclusion-and-future-challenges}{%
\subsection{Conclusion and Future
Challenges}\label{conclusion-and-future-challenges}}

This study constructed a recursive epistemic model based on the
hierarchical structure of knowledge. Using a single core function \(K\)
applied recursively---\(K(x)\), \(K(K(x))\), \(K(K(K(x)))\)---we provide
a mathematically rigorous yet philosophically grounded framework that
captures:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \textbf{recursive nature of self-awareness}: The same epistemic
  question applies at every level of reflection.
\item
  The \textbf{hierarchical structure of metacognition}: Distinguishing
  ``knowing ignorance'' (Socratic wisdom) from ``unknowing ignorance''
  (Dunning-Kruger effect).
\item
  The \textbf{continuous gradation of knowledge}: Knowledge states exist
  on a continuum from misconception (\(-1\)) through ignorance (\(0\))
  to accurate knowledge (\(1\)).
\end{enumerate}

\hypertarget{main-results}{%
\subsubsection{Main Results}\label{main-results}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We proposed a \textbf{layered observation model \(\{K_n\}\)} with
  formal anchor constraints (preservation, monotonicity, boundedness),
  demonstrating that recursive metacognition is a well-founded
  observational structure. The symbolic notation \(K(K(x))\) is retained
  for conceptual communication, while all formal work uses \(K_n\)
  exclusively.
\item
  We adopted a stance of \textbf{methodological relativism}, treating
  the proposition \(x\) itself as the implicit reference point and
  leaving the designation of ``correct'' to the experimental context.
\item
  We provided the \textbf{Four Quadrants of Metacognition}, clearly
  distinguishing ``Knowing Ignorance'' (Socratic wisdom,
  \(K_0(x)=0, K_1(x)=1\)) from ``Unknowing Ignorance'' (Dunning-Kruger
  effect, \(K_0(x)=0, K_1(x)=-1\)).
\item
  We separated \textbf{epistemic state} (\(K\)) from
  \textbf{phenomenological confidence} (\(C\)), recognizing them as
  orthogonal dimensions.
\item
  We proposed the \textbf{Metacognitive Alignment Test (MAT)} as an
  experimental protocol to validate the model, with specific predictions
  about the benefits of Socratic wisdom.
\end{enumerate}

\hypertarget{theoretical-contributions}{%
\subsubsection{Theoretical
Contributions}\label{theoretical-contributions}}

\begin{itemize}
\tightlist
\item
  \textbf{Recursive Formalization}: Extending metacognition to arbitrary
  depths while maintaining mathematical consistency
\item
  \textbf{Layered Observation Model}: Justifying recursive structure via
  indexed observations (\(K_n\)) with formal anchor constraints
\item
  \textbf{Socratic Wisdom as Achievement}: Explicitly modeling ``knowing
  ignorance'' as a high metacognitive state
\item
  \textbf{Per-Item Granularity}: Measuring metacognition at the
  individual item level, complementing aggregate statistical measures
\end{itemize}

\hypertarget{limitations}{%
\subsubsection{Limitations}\label{limitations}}

This framework is a \textbf{conceptual scaffold} for organizing
metacognitive phenomena, not a complete predictive model. We acknowledge
the following limitations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Formal Model:} We adopt an observational family interpretation
  to resolve the recursion/observation tension. This is a modeling
  choice, not the only possibility.
\item
  \textbf{Single Dimension (Scope Boundary):}

  \begin{itemize}
  \tightlist
  \item
    The \([-1,1]\) scale assumes a \textbf{single axis of correctness}
    with one ``opposite'' direction.
  \item
    \textbf{What this captures:} Directional errors (overconfidence vs
    underconfidence, correct vs incorrect).
  \item
    \textbf{What this does NOT capture:} Multiple, qualitatively
    different misconceptions (e.g., ``thinks A'' vs ``thinks B'' when
    truth is C).
  \end{itemize}

  \textbf{Formal Clarification:}

  The current model assumes a binary opposition:
  \(\text{Reference} \leftrightarrow \text{Anti-Reference}\)

  \[K_0 = \begin{cases}
  1 & \text{if Response} = \text{Reference} \\
  0 & \text{if Response} = \text{Absent} \\
  -1 & \text{if Response} \neq \text{Reference}
  \end{cases}\]

  This collapses all incorrect responses to \(-1\), regardless of
  \emph{which} error was made.

  \textbf{When This is Appropriate:}

  \begin{itemize}
  \tightlist
  \item
    Binary propositions (true/false)
  \item
    Single-answer factual questions
  \item
    Ordinal scales with clear direction
  \end{itemize}

  \textbf{When This is Limiting:}

  \begin{itemize}
  \tightlist
  \item
    Conceptual errors with multiple wrong paths
  \item
    Skill assessments with qualitatively different failure modes
  \item
    Open-ended responses
  \end{itemize}

  \textbf{Future Extension (Out of Scope):}

  For multi-dimensional misconceptions, consider:

  \[K_0: \mathcal{X} \to [-1, 1]^d\]

  Where \(d\) = number of independent error dimensions. This requires:

  \begin{itemize}
  \tightlist
  \item
    Multi-dimensional reference space
  \item
    Vector-valued embedding \(g_0\)
  \item
    Revised axioms for vector order
  \end{itemize}

  We leave this extension for future work, noting that the current
  scalar formulation covers a wide range of practical applications.
\item
  \textbf{Scope Boundary: Binary vs.~Graded Truth}

  \textbf{Current Scope}: This framework assumes \textbf{binary
  correctness} at \(K_0\):

  \begin{itemize}
  \tightlist
  \item
    Correct (\(K_0 = +1\))
  \item
    Absent (\(K_0 = 0\))
  \item
    Incorrect (\(K_0 = -1\))
  \end{itemize}

  \textbf{Out of Scope (Future Work)}:

  \begin{longtable}[]{@{}
    >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2619}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.2619}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4762}}@{}}
  \toprule\noalign{}
  \begin{minipage}[b]{\linewidth}\raggedright
  Extension
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Challenge
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Potential Approach
  \end{minipage} \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  \textbf{Partial Credit} & \(K_0 \in (0, 1)\) requires graded reference
  & Probabilistic \(f_0\) with continuous output \\
  \textbf{Multi-label} & Multiple correct answers & Set-valued \(K_0\)
  or soft-max embedding \\
  \textbf{Graded Truth} & Degrees of correctness & Fuzzy reference with
  \(K_0 = \text{similarity}(\text{Response}, \text{Reference})\) \\
  \end{longtable}

  \textbf{Why Binary for Now}: Binary correctness enables clean anchor
  semantics and unambiguous \(K_1\)/\(K_2\) computation. Graded
  extensions require principled definitions of ``partial alignment''
  that preserve interpretability.

  \textbf{Extension Path: Polytomous and Partial Credit Scoring}

  For polytomous outcomes (e.g., rubric scores 0, 1, 2, 3), we specify a
  concrete extension via the Graded Response Model (GRM):

  \[P(Y \geq k | \theta) = \frac{1}{1 + e^{-a(\theta - b_k)}}\]

  \textbf{Mapping to Graded \(K_0\)}:

  \[K_0^{(\text{graded})} = \frac{2Y - Y_{\max}}{Y_{\max}}\]

  Where \(Y\) is the observed score and \(Y_{\max}\) is the maximum
  possible score.

  \textbf{Impact on Higher Layers}:

  Graded \(K_0\) propagates to \(K_1\) with graded alignment:

  \begin{itemize}
  \tightlist
  \item
    ``I'm 80\% confident'' matches \(K_0 = 0.6\) → partial alignment
  \item
    ``I'm 80\% confident'' matches \(K_0 = -0.2\) → partial misalignment
  \end{itemize}

  \textbf{When to Use Each Approach}:

  \begin{longtable}[]{@{}
    >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3226}}
    >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6774}}@{}}
  \toprule\noalign{}
  \begin{minipage}[b]{\linewidth}\raggedright
  Scenario
  \end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
  Recommended Approach
  \end{minipage} \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  Factual Q\&A (binary correct/incorrect) & Trichotomous
  \(K_0 \in \{-1, 0, +1\}\) \\
  Essay grading (rubric-based) & Graded \(K_0\) via GRM mapping \\
  Programming (test case pass rate) & Proportion-based \(K_0 = 2p - 1\)
  where \(p\) = pass rate \\
  Multiple-choice with partial credit & GRM-based \(K_0\) \\
  \end{longtable}

  \textbf{Implementation Status}: Conceptually compatible with the
  framework; formal development deferred to future work.
\item
  \textbf{Informativeness Constraint}

  \textbf{Problem}: A subject could trivially achieve \(K_n = 0\) for
  all \(n\) by always claiming ``I'm not sure.'' This would satisfy the
  framework's consistency requirements without providing useful
  information.

  \textbf{Solution}: We distinguish between \textbf{legitimate
  uncertainty} and \textbf{uninformative hedging} via:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Response Distribution Analysis}:

    \begin{itemize}
    \tightlist
    \item
      If \(P(\text{Claim}_n = \text{"not sure"}) > 0.8\) across items,
      flag as potentially uninformative
    \item
      Legitimate uncertainty should correlate with item difficulty
    \end{itemize}
  \item
    \textbf{Coherence Check}:

    \begin{itemize}
    \tightlist
    \item
      Subjects with genuine uncertainty should show \(K_0\) variance
      (some correct, some incorrect)
    \item
      Subjects gaming the system show uniform ``not sure'' regardless of
      \(K_0\) distribution
    \end{itemize}
  \item
    \textbf{Incentive Design} (Experimental):

    \begin{itemize}
    \tightlist
    \item
      Proper scoring rules that penalize uninformative claims
    \item
      Reward calibration: higher payoff for confident-and-correct
      vs.~uncertain-and-correct
    \end{itemize}
  \end{itemize}

  \textbf{Informativeness Index}:
  \[\text{Informativeness}(K_n) = 1 - H(K_n)/H_{\max}\]

  Where \(H(K_n)\) is the entropy of the subject's \(K_n\) distribution.
  Low informativeness (high entropy, uniform distribution) combined with
  no correlation to \(K_{n-1}\) triggers a warning.
\item
  \textbf{Minimal Axiomatic Theory:} We provide basic constraints on
  \(K\) (anchor preservation, monotonicity, boundedness) but do not
  specify a unique functional form. The specific dynamics of \(K\)
  (e.g., whether it is contractive, has fixed points beyond
  \(\{-1, 0, 1\}\)) are empirical questions.
\item
  \textbf{No Generative Model:} We do not provide a noise model or
  generative account of how states are produced. This is a task for
  computational cognitive modeling.
\item
  \textbf{Observation Protocol:} The mapping from observable behavior to
  \(K_n\) values requires operational definitions. While we provide
  guidelines (e.g., alignment between claims and performance), the
  specific elicitation methods depend on the application domain.
\item
  \textbf{\(K_2\) Identifiability:} Higher-order estimates (\(K_2\),
  \(K_3\)) require more items and may be less reliable.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Guideline:} For reliable \(K_2\) estimation, use
    \(N \geq 50\) items with noise \(< 0.2\).
  \item
    \textbf{Confidence intervals:} Report 95\% CI via bootstrap; if CI
    width \(> 0.3\), interpret with caution.
  \end{itemize}
\item
  \textbf{Simulation Validation Pending:} We have not yet provided
  simulated evidence that different metacognitive profiles (Socratic,
  Dunning-Kruger, Imposter) are identifiable under realistic noise. This
  is planned for future work.
\item
  \textbf{Analogical Type Theory:} The type-theoretic justification is
  analogical rather than formally constructed. A full domain-theoretic
  or typed lambda-calculus treatment is beyond the current scope.
\item
  \textbf{LLM Operationalization Incomplete:} Applying \(K_n\) to LLMs
  requires addressing question-side shortcuts and model-side signals.
  Specific methods (conformal coverage, debate protocols) are suggested
  but not developed here.
\end{enumerate}

\hypertarget{validation-roadmap}{%
\subsubsection{Validation Roadmap}\label{validation-roadmap}}

This paper establishes the \textbf{conceptual framework}; validation is
planned for follow-up work. We present a staged validation program:

\hypertarget{phase-1-reliability}{%
\paragraph{Phase 1: Reliability}\label{phase-1-reliability}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Target
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Test-Retest} & 2-week interval, same items & \(r > 0.7\) \\
\textbf{Internal Consistency} & Cronbach's \(\alpha\) across items &
\(\alpha > 0.8\) \\
\textbf{Split-Half} & Odd-even item split & \(r > 0.75\) \\
\end{longtable}

\hypertarget{phase-2-convergent-validity}{%
\paragraph{Phase 2: Convergent
Validity}\label{phase-2-convergent-validity}}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\(K\) Measure & Comparison Metric & Expected Correlation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_1\) & meta-d'/d' & \(r > 0.6\) (positive) \\
\(K_1\) & Type-2 AUROC & \(r > 0.5\) (positive) \\
\(K_0\) & IRT ability \(\theta\) & \(r > 0.8\) (positive) \\
\end{longtable}

\hypertarget{phase-3-discriminant-validity}{%
\paragraph{Phase 3: Discriminant
Validity}\label{phase-3-discriminant-validity}}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
\(K\) Measure & Comparison & Expected \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_1\) & Raw confidence \(C\) & \(r < 0.3\) (low) \\
\(K_0\) & Response time & \(r < 0.2\) (low) \\
\end{longtable}

\hypertarget{phase-4-predictive-validity}{%
\paragraph{Phase 4: Predictive
Validity}\label{phase-4-predictive-validity}}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Predictor & Outcome & Hypothesis \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(K_1 = 1\) (Socratic) & Help-seeking & Higher \\
\(K_2 = 1\) & Intervention responsiveness & Higher \\
\(K_1 = -1\) (DK) & Overconfident errors & Higher \\
\end{longtable}

\textbf{Acknowledgment}: We recognize that this validation program is
\textbf{essential} for empirical adoption. The current paper's
contribution is the \textbf{conceptual and formal foundation} upon which
such validation can be built.

\hypertarget{future-directions}{%
\subsubsection{Future Directions}\label{future-directions}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Empirical Validation}: Conduct MAT experiments to validate the
  model's predictions about Socratic wisdom and Dunning-Kruger effect,
  and compare with meta-d' and calibration metrics.
\item
  \textbf{Simulation Studies}: Simulate agents with known \(K\)/\(C\)
  profiles to demonstrate identifiability and estimate required sample
  sizes.
\item
  \textbf{Formal Type Theory}: Develop a typed calculus or algebraic
  data type where \(K\)'s self-application is a well-typed endomorphism.
\item
  \textbf{AI Safety Applications}: Operationalize \(K(K(x))\) for LLMs
  using abstention behavior, self-consistency, and metamorphic testing.
\item
  \textbf{Cultural and Domain Variation}: Investigate whether the
  symmetry assumption (misconception vs.~knowledge) holds across
  cultures and domains.
\end{enumerate}

\hypertarget{why-re-separation-is-unnecessary}{%
\subsubsection{Why R/E Separation is
Unnecessary}\label{why-re-separation-is-unnecessary}}

One alternative formalization might introduce two separate maps: a
subject-level recognition/report map \(R\) and an evaluator/alignment
map \(E\). We argue that such separation is \textbf{unnecessary} for our
framework.

\textbf{The Misunderstanding:}

Such a proposal typically interprets \(K(K(x))\) as: 1. \(K(x)\) =
first-order state (a number) 2. \(K(K(x))\) = applying \(K\) to that
number 3. Therefore \(K(0)\) should equal \(0\)

\textbf{The Reality:}

Our \(K\) is purely \textbf{observational}: 1. \(K_0\) = observation of
State\(_0\) (first-order epistemic state) 2. \(K_1\) = observation of
State\(_1\) (metacognitive state) 3. \(K_2\) = observation of
State\(_2\) (meta-metacognitive state)

\textbf{Each layer observes a different object.} There is no ``subject's
report'' vs ``evaluator's assessment'' --- there is only
\textbf{observation of states}.

\textbf{Why R/E Adds Unnecessary Complexity:}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
R/E Approach & Our Approach \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Introduces ``subject'' as agent & No subject, only states \\
Requires modeling ``perception'' & States exist, observer measures \\
Two functions (R, E) & One function (\(K\)) \\
Subjective/objective split & Purely objective \\
\end{longtable}

\textbf{Our framework is simpler and more elegant because it does not
reify ``the subject'' as a separate entity with ``perceptions.''}

All that exists are \textbf{states} and \textbf{observations}. \(K\)
observes states. That's it.

\hypertarget{references}{%
\subsection{References}\label{references}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Kant, I. (1781). \emph{Critique of Pure Reason}.
\item
  Flavell, J. H. (1979). Metacognition and cognitive monitoring: A new
  area of cognitive-developmental inquiry. \emph{American Psychologist},
  \emph{34}(10), 906-911.
\item
  Nelson, T. O., \& Narens, L. (1990). Metamemory: A theoretical
  framework and new findings. In G. H. Bower (Ed.), \emph{The Psychology
  of Learning and Motivation} (Vol. 26, pp.~125-173). Academic Press.
\item
  Koriat, A. (1993). How do we know that we know? The accessibility
  model of the feeling of knowing. \emph{Psychological Review},
  \emph{100}(4), 609-639.
\item
  Kruger, J., \& Dunning, D. (1999). Unskilled and unaware of it: How
  difficulties in recognizing one's own incompetence lead to inflated
  self-assessments. \emph{Journal of Personality and Social Psychology},
  \emph{77}(6), 1121-1134.
\item
  Maniscalco, B., \& Lau, H. (2012). A signal detection theoretic
  approach for estimating metacognitive sensitivity from confidence
  ratings. \emph{Consciousness and Cognition}, \emph{21}(1), 422-430.
\item
  Shafer, G. (1976). \emph{A Mathematical Theory of Evidence}. Princeton
  University Press.
\item
  Fleming, S. M., \& Daw, N. D. (2017). Self-evaluation of
  decision-making: A general Bayesian framework for metacognitive
  computation. \emph{Psychological Review}, \emph{124}(1), 91-114.
\item
  Dayan, P. (2023). Metacognitive information theory. \emph{bioRxiv}.
  https://doi.org/10.1162/opmi\_a\_00091
\end{enumerate}

\end{document}
